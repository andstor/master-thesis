\chapter{Data}
\label{chap:datasets}
This chapter introduces the necessary background information for this study. First, a brief introduction to blockchain technology is provided in \cref{sec:blockchain} and then the concept of \acrfullpl{sc} is introduced in \cref{sec:smart-contract}. Finally, in \cref{sec:smart-contract-vulnerabilities}, the most popular \acrshort{sc} vulnerabilities are described.

\section{Smart contract downloader}
\label{sec:smart-contract-downloader}
\url{https://github.com/andstor/smart-contract-downloader}


The largest provider of verified \acrshortpl{sc} is Etherscan. This website provides a list of all verified \acrshortpl{sc} on the blockchain. More on their service...... Etherscan provides a API for downloading verified Smart Contracts. The API is available at \url{https://api.etherscan.io/api}.

In order to download the \acrshortpl{sc} from Etherscan, a tool we need to provide the \acrshortpl{sc} address. The address is the first part of the \acrshortpl{sc} code. The address is the first part of the \acrshortpl{sc} code.

The following code snippet is a Google BigQuery query. It will select all \acrshortpl{sc} addresses on the Ethereum blockchain that has at least one transaction. This query was run on the 1st of April 2022, and the result was downloaded as a CSV file, and is available at \url{https://huggingface.co/datasets/andstor/smart_contracts/blob/main/contract_addresses.csv}. The CSV file is then used to download the \acrshortpl{sc} from Etherscan.

\begin{lstlisting}[
    caption={Google BigQuery query for selecting all \acrlong{sc} addresses on Ethereum that has at least one transaction.},
    label=lst:reentrancy,
    language=SQL]
SELECT contracts.address, COUNT(1) AS tx_count
FROM `bigquery-public-data.crypto_ethereum.contracts` AS contracts
JOIN `bigquery-public-data.crypto_ethereum.transactions` AS transactions 
      ON (transactions.to_address = contracts.address)
GROUP BY contracts.address
ORDER BY tx_count DESC
}
\end{lstlisting}

\todo{Include img of the processing script output}
Saved to file for simple reestarrting, multiprocessing and parallelization.

The total number of files generated by the downloading program was 5,810,042. In order to efficiently process these, all files were combined into a tarfile. A processing script was then created for filtering out all "empty" files. These correspond to a contract address on Ethereum that has not been verified on Etherscan.io. A total of 3,592,350 files were empty, making the source code of 38,17\% of the deployed contracts on Ethereum available. Each non-empty file is then parsed and the contract data is extracted. This extraction process is rather complicated, as smart contract sources come in a wide variety of flavors and formats.

\subsection{Normalization}
The most common is a contract written the Solidity language with  a single contract "entry"  \todo{Find a better name for contract keyword}. However, a single contrract file can contain multtiple contracts, making use of properties like inheritance etc.. The source code contracts can also be split over multiple files, a formmat rreefered to as "Multi file". When compiling thtese, the source code files aree "flattened" into a single contract file before compiliattion. Anotther flavour is hte JSON format, which is a language that is used to describe the \acrshortpl{sc}. Here the sourcecode is structured in tthe in the JSON code. Smart contracts can also be vritten in the Vyper language. Vyper is .... \todo{explain vyper}.


\begin{lstlisting}[
    caption={Solidity standard JSON Input format.},
    label=lst:flattened-dataset-cmd,
    language=JSON]
{
    "sources": {/* ... */},
    "settings": {
        "optimizer": {/* ... */},
        "evmVersion": "<VERSION>"
    }
}
\end{lstlisting}

All of the above formats are processed by the processing script, normalizing the contract source code to a single "flattened" contract file. The source code, along with the contract metadata, is then saved across multiple Parquet files, each consisting of 30000 "flattened" contracts. A total of 2,217,692 smart contracts were successfully parsed and normalized.

\subsection{Duplication filtering}
\label{sec:duplication-filtering}
A large quantity of Smart Contracts contains duplicated code. Primarily, this is due to the frequent use of library code, such as Safemath and ... \todo{Reference libraries}. Etherscan requires the library code used in a contract to be embedded in the source code. Filtering is applied to produce a dataset with a mostly unique contract source code to mitigate this. This filtering is done by calculating the string distance between the source code. Due to the rather large amount of contracts (\~2 million), the comparison is only made within groups of contracts. These groups are defined by grouping on the "contract\_name" for the \textit{flattened} dataset, and by "file\_name" for the \textit{inflated} dataset. These datasets will be discssed in detail in the following sections.

The actual code filtering is done by applying a token-based similarity algorithm named Jacard Index. The algorithm is computationally efficient and can be used to filter out \acrshortpl{sc} that are not similar to the query. The Jacard Index is a measure of the similarity between two sets. The Jacard Index is defined as the ratio of the size of the intersection to the size of the union of the two sets.


\section{Datasets}
This section describes the datasets used and created in this study.

Describe the  PILE...  It consists of among others, a lot of data from GitHub. HHowever, only x\% of the data is smart contracts (Solidity). Hence there is a need for a dataset made up of smart contracts. --> existing datasets....

\subsection{Verified Smart Contracts}
\label{sec:verified-smart-contracts}
\url{https://github.com/andstor/verified-smart-contracts}
\url{https://huggingface.co/datasets/andstor/smart_contracts}

The Verified Smart Contracts dataset is a dataset consisting of verified Smart Contracts from Etherscan.io. This is real smmart contracts that are deployed to the Ethereum blockchain. A set of 100,000 to 200,000 contracts are provided, containing both Solidity and Vyper code.

\cref{tab:verified-smart-contracts-metrics} shows the metrics of the various (sub)datasets.

\begin{table}
    %\newcolumntype{Y}{>{\centering\arraybackslash}X}
    \def\arraystretch{1.5}
    \small
    \centering
    \caption{Verified Smart Contracts Metrics}
    \label{tab:verified-smart-contracts-metrics}
    \begin{tabularx}{\textwidth}{XXXX}
        \toprule
        \textbf{Component} & \textbf{Size} &  \textbf{Num rows} & \textbf{LoC*}\\
        \midrule
        Raw & 0.80 GiB & 2,217,692 & 839,665,295\\
        Flattened & 1.16 GiB & 136,969 & 97,529,473\\
        Inflated & 0.76 GiB & 186,397 & 53,843,305\\
        Parsed & 4.44 GiB & 4,434,014 & 29,965,185\\
        \bottomrule
    \end{tabularx}
\end{table}

LoC refers to the lines of source\_code. The Parsed dataset counts lines of func\_code + func\_documentation.

\subsubsection{Raw}
\label{sec:verified-smart-contracts-raw}
The raw dataset contains mostly the raw data from Etherscan, downloaded with the smart-contract-downlader tool, as described in \cref{sec:smart-contract-downloader}. All different contract formats (JSON, multi-file, etc.) are normalized to a flattened source code structure. 

\todo{Add stats on the raw dataset}

\subsubsection{Flattened}
\label{sec:verified-smart-contracts-flattened}

The flattened dataset is a filtered version  of the Raw dataset\cref{sec:verified-smart-contracts-raw}. It contains smart contracts, where every contract contains all required library code. Each "file" is marked in the source code with a comment stating the original file path: //File: path/to/file.sol. These are then filtered for uniqueness with a similarity threshold of 0.9. This means that all contracts whose code shares more than 90\% of the tokens will be discarded. The low uniqueness requirement is due to the often large amount of embedded library code. If the requirement is set to high, the actual contract code will be negligible compared to the library code. Most contracts will be discarded, and the resulting dataset would contain mostly unique library code. However, the dataset as a whole will have a large amount of duplicated libray code. From the 2,217,692 contracts, 2,080,723 duplications are found, giving a duplication percentage of 93.82\%. The resulting dataset consists of 136,969 contracts.

%Processing: 100%|██████| 74/74 [20:22<00:00, 16.51s/it, dupes=2081081/2217692 (93.84%)]

The following command prooduces the flattened dataset:

\lstinline[language=Python]!python script/filter_data.py -s parquet -o data/flattened --threshold 0.9!


\begin{lstlisting}[
    caption={Solidity standard JSON Input format.},
    label=lst:flattened-dataset-cmd,
    language=JSON]
{
  'contract_name': 'MiaKhalifaDAO',
  'contract_address': '0xb3862ca215d5ed2de22734ed001d701adf0a30b4',
  'language': 'Solidity',
  'source_code': '// File: @openzeppelin/contracts/utils/Strings.sol\r\n\r\n\r\n// OpenZeppelin Contracts v4.4.1 (utils/Strings.sol)\r\n\r\npragma solidity ^0.8.0;\r\n\r\n/**\r\n * @dev String operations.\r\n */\r\nlibrary Strings {\r\n...',
  'abi': '[{"inputs":[{"internalType":"uint256","name":"maxBatchSize_","type":"uint256"}...]',
  'compiler_version': 'v0.8.7+commit.e28d00a7',
  'optimization_used': False,
  'runs': 200,
  'constructor_arguments': '000000000000000000000000000000000000000000000000000000000000000a000...',
  'evm_version': 'Default',
  'library': '',
  'license_type': 'MIT',
  'proxy': False,
  'implementation': '',
  'swarm_source': 'ipfs://e490df69bd9ca50e1831a1ac82177e826fee459b0b085a00bd7a727c80d74089'
}
\end{lstlisting}



\subsubsection{Inflated}
\label{sec:verified-smart-contracts-inflated}
The inflated dataset is also based on the raw dataset. Each contract file in the dataset is split into its original representative files. This mitigates a lot of the problems of the flattened dataset in terms of duplicated library code. The library code would, along with other imported contract files, be split into separate contract records. The 2,217,692 "raw" smart contracts are inflated to a total of 5,403,136 separate contract files. These are then grouped by "file\_name" and filtered for uniqueness with a similarity threshold of 0.9. This should produce a dataset with a large amount of unique source code, with low quantities of library code. A total of 5,216,739 duplications are found, giving a duplication percentage of 96.56\%. The resulting dataset consists of 186,397 contracts.

%Processing: 100%|██████| 74/74 [22:50<00:00, 18.52s/it, dupes=5217191/5403136 (96.56%)]


\lstinline[language=Python]!python script/filter_data.py -s parquet -o data/inflated --split-files --threshold 0.9!
dupes=5217191/5403136 (96.56%)


\begin{lstlisting}[
    caption={Solidity standard JSON Input format.},
    label=lst:flattened-dataset-cmd,
    language=JSON]
    {
        'contract_name': 'PinkLemonade',
        'file_path': 'PinkLemonade.sol',
        'contract_address': '0x9a5be3cc368f01a0566a613aad7183783cff7eec',
        'language': 'Solidity',
        'source_code': '/**\r\n\r\nt.me/pinklemonadecoin\r\n*/\r\n\r\n// SPDX-License-Identifier: MIT\r\npragma solidity ^0.8.0;\r\n\r\n\r\n/*\r\n * @dev Provides information about the current execution context, including the\r\n * sender of the transaction and its data. While these are generally available...',
        'abi': '[{"inputs":[],"stateMutability":"nonpayable","type":"constructor"}...]',
        'compiler_version': 'v0.8.4+commit.c7e474f2',
        'optimization_used': False,
        'runs': 200,
        'constructor_arguments': '',
        'evm_version': 'Default',
        'library': '',
        'license_type': 'MIT',
        'proxy': False,
        'implementation': '',
        'swarm_source': 'ipfs://eb0ac9491a04e7a196280fd27ce355a85d79b34c7b0a83ab606d27972a06050c'
      }
      
      
\end{lstlisting}

\begin{figure}[ht]
    \centering
    \input{figures/security_levels.pgf}
    \caption{Doughnut chart over security levels, where each level occurs at least once in the \acrshort{sc}. The inner ring shows the distribution of the occurrences of each level. The outer ring shows the additional security levels for each contract. For example, "HML" means that the contract has at least three vulnerabilities with the corresponding "High", "Medium", and "Low" security levels.}
\end{figure}


\begin{figure}[ht]
    \centering
    \input{figures/vulnerabilities_bar.pgf}
    \caption{Histogram \small x of verified \acrshortpl{sc} on Ethereum}
\end{figure}

\subsubsection{Plain text}
\label{sec:verified-smart-contracts-plain-text}
For easy use of the dataset for casual language modeling training, a "plain\_text" version of both the raw, the flattened, and the inflated dataset is made available. This is done through a custom builder script for the dataset, a feature of the Dataset library by Hugging Face.

\subsubsection{Parsed}
\label{sec:verified-smart-contracts-parsed}

\subsection{Verified Smart Contracts Audit}
\label{sec:verified-smart-contracts-audit}
\url{https://github.com/andstor/verified-smart-contracts-audit}
\url{https://huggingface.co/datasets/andstor/smart_contracts_audit}

Subsets:
\subsubsection{SoliDetector}
\label{sec:verified-smart-contracts-audit-solidector}



\subsection{Smart Contract Comments}
\label{sec:verified-smart-contracts-comments}

\url{https://huggingface.co/datasets/andstor/smart_contract_comments}
See \cref{sec:code-comment-clustering} for more information.


\begin{figure}[ht]
    \centering
    \input{figures/2d_cluster_marginals.pgf}
    \caption{Histogram \small x of verified \acrshortpl{sc} on Ethereum}
\end{figure}
