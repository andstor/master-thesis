\chapter{Introduction}
\cref{chap:introduction}
The art of computer programming is an ever-evolving field. The field has transformed from punchcards to writing assembly code. With the introduction of the C programming language, the field sky-rocketed. Since then, a number of new languages have been introduced, and the art of programming has become a complex and ever-changing field. Today, computer systems are all around us and permeates every aspect of our lives. However, constructing such systems is a hard and time-consuming task. A number of tools and methods have been developed to increase the productivity of programmers, as well as to making programming more accessible to everyone. 

Recent advancements in large-scale transformer-based language models have successfully been used for generating code. Automatic code generation is a new and exciting technology that opens up a new world of possibilities for software developers. One example is GitHub Copilot \cite{copilot}. Copilot uses these models to generate code for a given programming language. The tool is based on a deep learning model, named Codex \cite{chen2021codex} by OpenAI, that has been trained on a large corpus of code. This enables developers to significantly speed up productivity. In addition, it makes programming more accessible to everyone by significantly reducing the threshold for using various language syntax' and libraries. Another recent contribution is AlphaCode \cite{alphacode}, a code generation tool for generating novel code solutions to programming competitions.

The language models are getting larger and better by the day. However, it is just as important \textit{how} these models are best put to use. Describing functionality is easy. Implementing it in code is hard. Almost every coding language supports some form of code comment. These are normally created to explain the implemented code after the code is written. Recent works \cite{chen2021codex,colin2020pymt5} have leveraged the power of transformers to automatically generate code from comments. This has the potential to greatly lower the threshold for non-developers to leverage programming, while also increasing efficiency due to a simpler developing process. However, none of these works investigates how to best write these comments for guiding code generation. \todo{Is this ok?} \hl{Further, for evaluating these systems, they only consider generating code from comments in isolation. This is rarely the case in a real-world setting. This is a very limited approach as the potential search space is enormous.} This thesis investigates a comment-aided approach to generating code, analyzing both comment style and the importance of context.

A machine learning model is only as good as the data it is fed. These large-scale transformers need huge amounts of data to be trained. Normally, this data is collected from all available open source code. A problem with this is that a lot of this code contains security problems. This can be everything from exposed API keys, to exploitable vulnerabilities. Autocomplete tools like GitHub Copilot must therefore be used with caution \cite{chen2021codex}.

To better secure automatic code generation, this thesis also purposes a novel approach for use of \acrshort{ml} models in large-scale transformer-based code generation pipelines to ensure secure generated code. To demonstrate the approach, this thesis will focus on generating secure code for \acrfullpl{sc} (Solidity). \acrfullpl{sc} have an exceptionally high demand for security, as vulnerabilities can not be fixed after a contract is deployed. Due to most blockchains' monetary and anonymous nature, they pose as a desirable target for adversaries and manipulators \cite{atzei2017survey}. Further, \acrshortpl{sc} tends to be rather short and simple, making it a good fit for generated code. The main research questions addressed in this thesis are:
\begin{itemize}
    \item How to automatically generate \acrlong{sc} code with transformer-based language models, by inputting comments to guide the code generation?
    \item How to generate secure \acrlong{sc} code with transformer-based language models?
\end{itemize}

\noindent
The specific contributions of this thesis are as follows:
\begin{itemize}
    \item The currently largest \acrfull{sc} dataset.
    \item Fine-tuned transformer-based language model for \acrlong{sc} code generation.
    \item Fine-tuned transformer-based language model for \acrlong{sc} code generation.
    \item Novel secure code generation method.
    \item Identification of open issues, possible solutions to mitigate these issues, and future directions to advance the state of research in the domain.
\end{itemize}

The rest of this paper is organized as follows. \cref{chap:background} describes the background of the project. The research related to this document is commented in \cref{chap:related-work}. \cref{chap:research-method} describes the methods used to implement the secure code generation. \cref{chap:results} describes the results of the project, and \cref{chap:discussion} discuss the findings. Identified future work is presented in \cref{chap:future-work}. \cref{chap:conclusion} presents final remarks and concludes the thesis.
