\chapter{Introduction}
\label{chap:introduction}
The art of computer programming is an ever-evolving field. The field has transformed from punchcards to writing assembly code. With the introduction of the C programming language, the field sky-rocketed. Since then, many new languages have been introduced, and the art of programming has become a complex and ever-changing field. Today, computer systems are all around us and permeate every aspect of our lives. However, constructing such systems is a hard and time-consuming task. Several tools and methods have been developed to increase the productivity of programmers, as well as to make programming more accessible to everyone. 

Recent advancements in large-scale transformer-based language models have successfully been used for generating code. Automatic code generation is a new and exciting technology that opens up a new world of possibilities for software developers. One example is GitHub Copilot \cite{copilot}. Copilot uses these models to generate code for a given programming language. The tool is based on a deep learning model, named Codex \cite{chen2021codex} by OpenAI, that has been trained on a large corpus of code. This enables developers to significantly speed up productivity. In addition, it makes programming more accessible to everyone by significantly reducing the threshold for using various programming languages and libraries. Another recent contribution is AlphaCode \cite{alphacode}, a code generation tool for generating novel code solutions to programming competitions.

The language models are getting larger and better by the day. However, it is just as important \textit{how} these models are best put to use. Describing functionality is easy. Implementing it in code is hard. Almost every coding language supports some form of code comment. These are normally created to explain the implemented code after the code is written. Recent works \cite{chen2021codex,colin2020pymt5} have leveraged the power of transformers to automatically generate code from comments. This has the potential to greatly lower the threshold for non-developers to leverage programming, while also increasing efficiency due to a simpler developing process. However, none of these works investigates how to best write these comments for guiding code generation. Further, for evaluating these systems, they only consider generating code from comments in isolation. This is rarely the case in a real-world setting because developers write code and use comments to explain the code. This thesis investigates a comment-aided approach to generating code, meaning the use of the existing code and comments as the input. For generating code, this work does not exclude the opportunity for developers to type in code and comments in combination with the automatically generated code. 

A machine learning model is only as good as the data it is fed. These large-scale transformers need huge amounts of data to be trained. Normally, this data is collected from all available open source code. A problem with this is that a lot of this code contains security problems. This can be everything from exposed API keys, to exploitable vulnerabilities. Autocomplete tools like GitHub Copilot must therefore be used with caution \cite{chen2021codex}.

To better secure automatic code generation, this thesis also purposes a novel approach for use of \acrshort{ml} models in large-scale transformer-based code generation pipelines to ensure secure generated code. To demonstrate the approach, this thesis will focus on generating secure code for \acrfullpl{sc} (Solidity). \acrfullpl{sc} have an exceptionally high demand for security, as vulnerabilities can not be fixed after a contract is deployed. Due to most blockchains' monetary and anonymous nature, they pose as a desirable target for adversaries and manipulators \cite{atzei2017survey}. Further, \acrshortpl{sc} tends to be rather short and simple, making it a good fit for generated code. The main research questions addressed in this thesis are:
\begin{itemize}
    \item How to automatically generate \acrlong{sc} code with transformer-based language models, by inputting comments to guide the code generation?
    \item How to generate secure \acrlong{sc} code with transformer-based language models?
\end{itemize}

\noindent
The specific contributions of this thesis are as follows:
\begin{itemize}
    \item The currently largest \acrfull{sc} dataset.
    \item Novel comment-aided code generation using the fine-tuned transformer-based language model to generate \acrlong{sc} code.
    \item Novel method to secure code generation.
    \item Identification of open issues, possible solutions to mitigate these issues, and future directions to advance the state of research in the domain.
\end{itemize}

The rest of this paper is organized as follows. \cref{chap:background} describes the background of the project. The research related to this document is commented in \cref{chap:related-work}. \cref{chap:research-methodoloy} describes the research methodology employed for implementing the research questions. The implementation and the results are presented in \cref{chap:implementation-and-results}. The implementations are then evaluated in \cref{chap:evaluation}. The findings and results from the implementation and evaluation are discussed in \cref{chap:discussion}. Finally, \cref{chap:conclusion} concludes the thesis, presenting final remarks and future works.
