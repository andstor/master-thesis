% Or call it Research results?

\chapter{Research Implementation and Results}
\label{chap:implementation-and-results}
This chapter presents the research implementation and results of the research questions. The chapter is divided into two parts. First, the implementation of research question 1 is described, concerning automatic smart contract code synthesis. The part of the chapter describes the implementation of research question 2, regarding generating secure smart contract code.

\section{Implementation of RQ1}
This section presents the implementation of research question 1. The implementation is done with the following steps:
\begin{enumerate}
    \item Create verified smart contract source code dataset.
    \begin{enumerate}
        \item Scrape verified smart contracts from the Ethereum blockchain.
        \item Normalize the smart contract files.
        \item Filter scraped verified smart contracts for uniqueness.
    \end{enumerate}
    \item Code comment analysis.
    \begin{enumerate}
        \item Create a parser that can parse all contract versions.
        \item Parse verified smart contract source code.
        \item Create a parsed dataset containing "comment, function" pairs.
        \item Cluster comments.
    \end{enumerate}
    \item Language modeling
    \begin{enumerate}
        \item Fine-tune a transformer model on the verified smart contracts dataset.
    \end{enumerate}
\end{enumerate}

\subsection{Data collection}
\label{sec:data-collection}

\subsubsection{Smart contract downloader}
\label{sec:smart-contract-downloader}
\url{https://github.com/andstor/smart-contract-downloader}


The largest provider of verified \acrshortpl{sc} is Etherscan. This website provides a list of all verified \acrshortpl{sc} on the blockchain. More on their service...... Etherscan provides a API for downloading verified Smart Contracts. The API is available at \url{https://api.etherscan.io/api}.

In order to download the \acrshortpl{sc} from Etherscan, a tool we need to provide the \acrshortpl{sc} address. The address is the first part of the \acrshortpl{sc} code. The address is the first part of the \acrshortpl{sc} code.

The following code snippet is a Google BigQuery query. It will select all \acrshortpl{sc} addresses on the Ethereum blockchain that has at least one transaction. This query was run on the 1st of April 2022, and the result was downloaded as a CSV file, and is available at \url{https://huggingface.co/datasets/andstor/smart_contracts/blob/main/contract_addresses.csv}. The CSV file is then used to download the \acrshortpl{sc} from Etherscan.

\begin{lstlisting}[
    caption={Google BigQuery query for selecting all \acrlong{sc} addresses on Ethereum that has at least one transaction.},
    label=lst:reentrancy,
    language=SQL]
SELECT contracts.address, COUNT(1) AS tx_count
FROM `bigquery-public-data.crypto_ethereum.contracts` AS contracts
JOIN `bigquery-public-data.crypto_ethereum.transactions` AS transactions 
      ON (transactions.to_address = contracts.address)
GROUP BY contracts.address
ORDER BY tx_count DESC
}
\end{lstlisting}

\todo{Include img of the processing script output}
Saved to file for simple restarting, multiprocessing and parallelization.

The total number of files generated by the downloading program was 5,810,042. In order to efficiently process these, all files were combined into a tarfile. A processing script was then created for filtering out all "empty" files. These correspond to a contract address on Ethereum that has not been verified on Etherscan.io. A total of 3,592,350 files were empty, making the source code of 38,17\% of the deployed contracts on Ethereum available. Each non-empty file is then parsed and the contract data is extracted. This extraction process is rather complicated, as smart contract sources come in a wide variety of flavors and formats.

\paragraph{Normalization}
The most common is a contract written the Solidity language with  a single contract "entry"  \todo{Find a better name for contract keyword}. However, a single contrract file can contain multtiple contracts, making use of properties like inheritance etc.. The source code contracts can also be split over multiple files, a formmat rreefered to as "Multi file". When compiling thtese, the source code files aree "flattened" into a single contract file before compiliattion. Anotther flavour is hte JSON format, which is a language that is used to describe the \acrshortpl{sc}. Here the sourcecode is structured in tthe in the JSON code. Smart contracts can also be vritten in the Vyper language. Vyper is .... \todo{explain vyper}.


\begin{lstlisting}[
    caption={Solidity standard JSON Input format.},
    label=lst:flattened-dataset-cmd,
    language=JSON]
{
    "sources": {/* ... */},
    "settings": {
        "optimizer": {/* ... */},
        "evmVersion": "<VERSION>"
    }
}
\end{lstlisting}

All of the above formats are processed by the processing script, normalizing the contract source code to a single "flattened" contract file. The source code, along with the contract metadata, is then saved across multiple Parquet files, each consisting of 30000 "flattened" contracts. A total of 2,217,692 smart contracts were successfully parsed and normalized.

\paragraph{Duplication filtering}
\label{sec:duplication-filtering}
A large quantity of Smart Contracts contains duplicated code. Primarily, this is due to the frequent use of library code, such as Safemath and ... \todo{Reference libraries}. Etherscan requires the library code used in a contract to be embedded in the source code. Filtering is applied to produce a dataset with a mostly unique contract source code to mitigate this. This filtering is done by calculating the string distance between the source code. Due to the rather large amount of contracts (\~2 million), the comparison is only made within groups of contracts. These groups are defined by grouping on the "contract\_name" for the \textit{flattened} dataset, and by "file\_name" for the \textit{inflated} dataset. These datasets will be discssed in detail in the following sections.

The actual code filtering is done by applying a token-based similarity algorithm named Jacard Index. The algorithm is computationally efficient and can be used to filter out \acrshortpl{sc} that are not similar to the query. The Jacard Index is a measure of the similarity between two sets. The Jacard Index is defined as the ratio of the size of the intersection to the size of the union of the two sets.

\subsubsection{Verified Smart Contracts}
\label{sec:verified-smart-contracts}
\url{https://github.com/andstor/verified-smart-contracts}
\url{https://huggingface.co/datasets/andstor/smart_contracts}

The Verified Smart Contracts dataset is a dataset consisting of verified Smart Contracts from Etherscan.io. This is real smmart contracts that are deployed to the Ethereum blockchain. A set of 100,000 to 200,000 contracts are provided, containing both Solidity and Vyper code.

\cref{tab:verified-smart-contracts-metrics} shows the metrics of the various (sub)datasets.

\begin{table}
    %\newcolumntype{Y}{>{\centering\arraybackslash}X}
    \def\arraystretch{1.5}
    \small
    \centering
    \caption{Verified Smart Contracts Metrics}
    \label{tab:verified-smart-contracts-metrics}
    \begin{tabularx}{\textwidth}{XXXX}
        \toprule
        \textbf{Component} & \textbf{Size} &  \textbf{Num rows} & \textbf{LoC*}\\
        \midrule
        Raw & 0.80 GiB & 2,217,692 & 839,665,295\\
        Flattened & 1.16 GiB & 136,969 & 97,529,473\\
        Inflated & 0.76 GiB & 186,397 & 53,843,305\\
        Parsed & 4.44 GiB & 4,434,014 & 29,965,185\\
        \bottomrule
    \end{tabularx}
\end{table}
\todo{Removeparsed from table \cref{tab:verified-smart-contracts-metrics}}
LoC refers to the lines of source\_code. The Parsed dataset counts lines of func\_code + func\_documentation.

\paragraph{Raw}
\label{sec:verified-smart-contracts-raw}
The raw dataset contains mostly the raw data from Etherscan, downloaded with the smart-contract-downlader tool, as described in \cref{sec:smart-contract-downloader}. All different contract formats (JSON, multi-file, etc.) are normalized to a flattened source code structure. 

\todo{Add stats on the raw dataset}

\paragraph{Flattened}
\label{sec:verified-smart-contracts-flattened}

The flattened dataset is a filtered version  of the Raw dataset\cref{sec:verified-smart-contracts-raw}. It contains smart contracts, where every contract contains all required library code. Each "file" is marked in the source code with a comment stating the original file path: //File: path/to/file.sol. These are then filtered for uniqueness with a similarity threshold of 0.9. This means that all contracts whose code shares more than 90\% of the tokens will be discarded. The low uniqueness requirement is due to the often large amount of embedded library code. If the requirement is set to high, the actual contract code will be negligible compared to the library code. Most contracts will be discarded, and the resulting dataset would contain mostly unique library code. However, the dataset as a whole will have a large amount of duplicated libray code. From the 2,217,692 contracts, 2,080,723 duplications are found, giving a duplication percentage of 93.82\%. The resulting dataset consists of 136,969 contracts. \cref{lst:flattened-dataset-instance} shows an example data instance from the dataset.

%Processing: 100%|██████| 74/74 [20:22<00:00, 16.51s/it, dupes=2081081/2217692 (93.84%)]

The following command prooduces the flattened dataset:

\lstinline[language=Python]!python script/filter_data.py -s parquet -o data/flattened --threshold 0.9!


\begin{lstlisting}[
    caption={Solidity standard JSON Input format.},
    label=lst:flattened-dataset-instance,
    language=JSON]
{
  'contract_name': 'MiaKhalifaDAO',
  'contract_address': '0xb3862ca215d5ed2de22734ed001d701adf0a30b4',
  'language': 'Solidity',
  'source_code': '// File: @openzeppelin/contracts/utils/Strings.sol\r\n\r\n\r\n// OpenZeppelin Contracts v4.4.1 (utils/Strings.sol)\r\n\r\npragma solidity ^0.8.0;\r\n\r\n/**\r\n * @dev String operations.\r\n */\r\nlibrary Strings {\r\n...',
  'abi': '[{"inputs":[{"internalType":"uint256","name":"maxBatchSize_","type":"uint256"}...]',
  'compiler_version': 'v0.8.7+commit.e28d00a7',
  'optimization_used': False,
  'runs': 200,
  'constructor_arguments': '000000000000000000000000000000000000000000000000000000000000000a000...',
  'evm_version': 'Default',
  'library': '',
  'license_type': 'MIT',
  'proxy': False,
  'implementation': '',
  'swarm_source': 'ipfs://e490df69bd9ca50e1831a1ac82177e826fee459b0b085a00bd7a727c80d74089'
}
\end{lstlisting}

\paragraph{Inflated}
\label{sec:verified-smart-contracts-inflated}
The inflated dataset is also based on the raw dataset. Each contract file in the dataset is split into its original representative files. This mitigates a lot of the problems of the flattened dataset in terms of duplicated library code. The library code would, along with other imported contract files, be split into separate contract records. The 2,217,692 "raw" smart contracts are inflated to a total of 5,403,136 separate contract files. These are then grouped by "file\_name" and filtered for uniqueness with a similarity threshold of 0.9. This should produce a dataset with a large amount of unique source code, with low quantities of library code. A total of 5,216,739 duplications are found, giving a duplication percentage of 96.56\%. The resulting dataset consists of 186,397 contracts. \cref{lst:inflated-dataset-instance} shows an example data instance from the dataset.

%Processing: 100%|██████| 74/74 [22:50<00:00, 18.52s/it, dupes=5217191/5403136 (96.56%)]


\lstinline[language=Python]!python script/filter_data.py -s parquet -o data/inflated --split-files --threshold 0.9!
dupes=5217191/5403136 (96.56%)


\begin{lstlisting}[
    caption={Solidity standard JSON Input format.},
    label=lst:lst:inflated-dataset-instance,
    language=JSON]
    {
        'contract_name': 'PinkLemonade',
        'file_path': 'PinkLemonade.sol',
        'contract_address': '0x9a5be3cc368f01a0566a613aad7183783cff7eec',
        'language': 'Solidity',
        'source_code': '/**\r\n\r\nt.me/pinklemonadecoin\r\n*/\r\n\r\n// SPDX-License-Identifier: MIT\r\npragma solidity ^0.8.0;\r\n\r\n\r\n/*\r\n * @dev Provides information about the current execution context, including the\r\n * sender of the transaction and its data. While these are generally available...',
        'abi': '[{"inputs":[],"stateMutability":"nonpayable","type":"constructor"}...]',
        'compiler_version': 'v0.8.4+commit.c7e474f2',
        'optimization_used': False,
        'runs': 200,
        'constructor_arguments': '',
        'evm_version': 'Default',
        'library': '',
        'license_type': 'MIT',
        'proxy': False,
        'implementation': '',
        'swarm_source': 'ipfs://eb0ac9491a04e7a196280fd27ce355a85d79b34c7b0a83ab606d27972a06050c'
      }
      
      
\end{lstlisting}


\paragraph{Plain text}
\label{sec:verified-smart-contracts-plain-text}
For easy use of the dataset for casual language modeling training, a "plain\_text" version of both the raw, the flattened, and the inflated dataset is made available. This is done through a custom builder script for the dataset, a feature of the Dataset library by Hugging Face.

\FloatBarrier

\subsection{Comment analysis}
\label{sec:comment-analysis}
To provide some insight into how a user can best formulate a comment for guiding the code synthesis, a cluster analysis of the comments in the smart contract dataset is conducted. First, a universal Solidity parser is constructed for parsing the Solidity code and extracting "code, comment" pairs. These results are then packaged into a dataset, and a clustering analysis is conducted. The results from this analysis are then later used in the evaluation of the code synthesis in the \ref{chap:evaluation}, shedding some light on which commenting style is the best to use.

\subsubsection{Universal Solidity parser}
\label{sec:universal-solidity-parser}
For being able to parse the Solidity \acrshort{sc}, a Solidity parser is constructed. This parser has to be universally compatible with all Solidity versions, hence the grammar used needs to be a lot less restrictive than the current official Solidity grammar available from Ethereum \cite{soliditygrammar}. ANTLR4 \cite{antlr4} is used for constructing the parser. ANTLR is a parser generator. By providing ANTLR with a formal language description called grammar, it can generate a complete parser that can automatically build parse trees. Parse trees are data structures representing how the grammar matches the input. Specifically, ANTLR4 generates a LL(*) (Left-to-right, leftmost derivation) parser \cite{parr2011llstar}. ANTLER is primarily a Java application. However, several code generation targets are available, including Java, C\#, Python, JavaScript, Go, C++, Swift, PHP and Dart \cite{antlr-targets}. In this project, the Python target is used.

Most programming language grammars available do not devote much effort to the handling of code comments. Comments are seen as unnecessary clutter and are normally discarded during lexing. For extracting the comments from the Solidity \acrshort{sc} code, the original source \cite{solidity-antlr4} for the official Solidity grammar \cite{soliditygrammar} is used. This old version is less restrictive and serves as a better starting point for ensuring support for all Solidity versions. This grammar is then simplified and made less restrictive, as well as adapted to support comments. \cref{fig:solidity-railroad-diagram} shows a railroad diagram of a subset of the main grammar rules altered for supporting comments. The complete universal Solidity parser is made available at https://github.com/andstor/solidity-universal-parser.

\begin{figure}[htbp]
    \centering
    %\makebox[\textwidth][c]{\includegraphics{figures/solidity-comments-railroad-diagram.pdf}}%
    \includegraphics[width=\textwidth]{figures/solidity-comments-railroad-diagram.pdf}
    \caption{Railroad diagrams of main code comment alteration to Solidity grammar.}
    \label{fig:solidity-railroad-diagram}
\end{figure}


\subsubsection{Verified Smart Contract Code Comments}
\label{sec:verified-smart-contracts-comments}
\url{https://huggingface.co/datasets/andstor/smart_contract_comments}

For doing the actual extraction of the "code, comment" pairs from the inflated version of the Verified Smart Contracts dataset (see \cref{sec:verified-smart-contracts-inflated}), the well-known visitor pattern \cite{visitor-pattern} is used for visiting the parse tree generated by the universal Solidity parser. ANTLER provides basic infrastructure for implementing such a visitor. The full implementation of the visitor is available at \url{https://github.com/andstor/verified-smart-contracts/blob/main/script/comment_visitor.py}. A script leveraging multiprocessing is used to parallelize the parsing of the dataset. See \url{https://github.com/andstor/verified-smart-contracts} for instructions on how to use this script. The resulting data is then filtered for functions that do not have code comments. These are simply removed and the result is then packaged as a new dataset named Verified Smart Contract Code Comments. A total of ...\todo{Add stats for comments dataset}. \cref{lst:comments-dataset-instance} shows an example data instance from the dataset.

%The results are then packaged and added to the Verified Smart Contracts dataset collection (\cref{sec:verified-smart-contracts}) under the name "parsed".Verified Smart Contracts dataset collection (\cref{sec:verified-smart-contracts}).

%Parsed: Total 4434014 functions, 4.44 GiB and 29965185 lines of code
%\lstinline[language=Python]!python script/parse_data.py -s data/inflated -o data/parsed!

\begin{lstlisting}[
    caption={Solidity standard JSON Input format.},
    label=lst:comments-dataset-instance,
    language=JSON]
{
    'contract_name': 'BondedECDSAKeep',
    'file_path': '@keep-network/keep-core/contracts/StakeDelegatable.sol',
    'contract_address': '0x61935dc4ffc5c5f1d141ac060c0eef04a792d8ee',
    'language': 'Solidity',
    'class_name': 'StakeDelegatable',
    'class_code': 'contract StakeDelegatable {\n    using OperatorParams for uint256;\n\n    mapping(address => Operator) internal operators;\n\n    struct Operator {\n        uint256 packedParams;\n        address owner;\n        address payable beneficiary;\n        address authorizer;\n    }\n\n...',
    'class_documentation': '/// @title Stake Delegatable\n/// @notice A base contract to allow stake delegation for staking contracts.',
    'class_documentation_type': 'NatSpecSingleLine',
    'func_name': 'balanceOf',
    'func_code': 'function balanceOf(address _address) public view returns (uint256 balance) {\n        return operators[_address].packedParams.getAmount();\n    }',
    'func_documentation': '/// @notice Gets the stake balance of the specified address.\n/// @param _address The address to query the balance of.\n/// @return An uint256 representing the amount staked by the passed address.',
    'func_documentation_type': 'NatSpecSingleLine',
    'compiler_version': 'v0.5.17+commit.d19bba13',
    'license_type': 'MIT',
    'swarm_source': 'bzzr://63a152bdeccda501f3e5b77f97918c5500bb7ae07637beba7fae76dbe818bda4'
}  
\end{lstlisting}

\subsubsection{Comment clustering}
\label{sec:comment-clustering}
This section is devoted to the clustering of the comments in the parsed dataset. The comments in the dataset are first preprocessed. In contrast to normal code, code comments are of a more natural language style. Normal natural language text preprocessing is therefore employed. First, the comments are lowercased and tokenized. The default English configuration of the \lstinline[language=Python]!word_tokenize! function from the popular NLTK (Natural Language Toolkit) \cite{nltk} python library is used for tokenization. Stemming is applied to the tokenized words, using the Porter Stemmer algorithm.

For converting the tokenized comments into word embeddings, both the word2vec algorithm \cite{word2vec} and \acrfull{tfidf} is used. The word2vec is able to capture some semantic similarities between the words. In particular, the implementation provided by the gensim library \cite{gensim} is used. The algorithm is configured to produce 100-dimensional vectors, using a window size of 5, and a minimum count of 5. 
%Word2Vec min_count=5,
%                     window=5,
%                     vector_size=100,
%                     sample=0.001,
%                     seed=1,
%                     alpha=0.025,
%                     min_alpha=0.0001, 
%                     negative=5,

To weed out the most frequent words \acrfull{tfidf} (see \cref{sec:tf-idf}) is also applied. For example, the different commenting types all start each line with a special word, such as "//", "///" or "*". By using \acrshort{tfidf}, it is possible to get more insights into the different ways of writing comments, beyond just the formatting style of the comments. The resulting word embeddings from the word2vec and \acrshort{tfidf} are multiplied. For each comment, the resulting word embeddings are averaged to form a final comment (or document) embedding.

The comment embeddings are clustered using the K-means algorithm. The number of clusters \(k\) is determined by using the Elbow method for deciding the optimal number of clusters. The results from the Elbow method are presented in \cref{fig:elbow}. From the curve, it is not entirely obvious where the "elbow" is. However, a \(k\) of 4 is selected. For visually inspecting the clustered comments result, the 100-dimensional vectors are reduced to 2D using \acrfull{pca}. The clustering result is shown in \cref{fig:comment-clusters}. The explained variance captured in the 2D plot is approximately 0.64, as shown in the Scree Plot in \cref{fig:scree-plot}.

%Word2Vec lifecycle event {'msg': 'training on 1431483720 raw words (762458270 effective words) took 782.9s, 973910 effective words/s', 'datetime': '2022-06-07T00:40:05.999678', 'gensim': '4.2.0', 'python': '3.7.13 (default, Apr 24 2022, 01:04:09) \n[GCC 7.5.0]', 'platform': 'Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}
%Time to train the model: 13.05 mins

\begin{figure}[htbp]
    \centering
    \input{figures/elbow.pgf}
    \caption{Elbow method for determining the optimal number of clusters.}
    \label{fig:elbow}
\end{figure}

\begin{figure}[htbp]
    \centering
    \input{figures/pca_explained_variance.pgf}
    \caption{Scree Plot for the PCA dimensionality reduction}
    \label{fig:scree-plot}
\end{figure}


\begin{figure}[htbp]
    \centering
    \input{figures/2d_cluster_marginals.pgf}
    \caption{2D plot of the comment clusters.}
    \label{fig:comment-clusters}
\end{figure}

\cref{lst:comment-cluster-0.1,lst:comment-cluster-0.2,lst:comment-cluster-1.1,lst:comment-cluster-1.2,lst:comment-cluster-2.1,lst:comment-cluster-2.2,lst:comment-cluster-3.1,lst:comment-cluster-3.2} shows two examples from each of the four clusters. Upon manual inspection of the different clusters, several patterns emerge. The cluster 0 is mainly composed of comments that is almost exclusively made up of NatSpec comments with only NatSpec fields, for example the "@parameter" and "@return" fields. Most comments also start with a brief description of the function, as for example line 1 in \cref{lst:comment-cluster-0.1}. Next, cluster 1 consists of one-liners, briefly describing what the function. Cluster 2 contains more lengthy comments that describe the function in detail. It is similar to cluster 1 but does not make significant use of the NatSpec fields. Most of these comments also add some variation of a "requirements" section, like the one in lines 4-7 in \cref{lst:comment-cluster-2.1}. Further, several of these comments are from some implementation of common libraries. For example, \cref{lst:comment-cluster-2.1} shows a comment for the implementation of a transfer function in a contract implementation of a ERC20 token. Compared to the base implementation by the OpenZeppelin library \cite{openzeppelin}, this version adds 1.7\% tax if the sender or recipient is an exchange (lines 8-10). Finally, cluster 3 contains very short comments that are more categorical by nature. For example, \cref{lst:comment-cluster-3.1} marks the function as "admin functions" and \cref{lst:comment-cluster-3.2} as "Deposit Management".


% Natspec with almost exclusive use of parameter and return keywords.
\begin{lstlisting}[
    caption={NatSpec single-line comment in cluster 0.},
    label=lst:comment-cluster-0.1,
    language=Solidity]
/// @dev Executes the next transaction only if the cooldown has passed and the transaction has not expired
/// @param to Destination address of module transaction
/// @param value Ether value of module transaction
/// @param data Data payload of module transaction
/// @param operation Operation type of module transaction
/// @notice The txIndex used by this function is always 0
\end{lstlisting}
\begin{lstlisting}[
    caption={NatSpec multi-line comment in cluster 0.},
    label=lst:comment-cluster-0.2,
    language=Solidity]
/**
 * @dev Gets the timestamp for the value based on ther index
 * @param _requestID is the requestId to look up
 * @param _index is the value index to look up
 * @return uint timestamp
 */   
\end{lstlisting}
% One-liners describing briefly the function.
\begin{lstlisting}[
    caption={Single-line comment in cluster 1.},
    label=lst:comment-cluster-1.1,
    language=Solidity]
// Allow the owner to cash out the holdings of this contract.
\end{lstlisting}
\begin{lstlisting}[
    caption={Multi-line comment in cluster 1.},
    label=lst:comment-cluster-1.2,
    language=Solidity]
/* Add a contract to our list of account minters */
\end{lstlisting}

% Library function implementations. The functions are clearly written, often stating  the requirements of the function. Written out in natural  language.
\begin{lstlisting}[
    caption={NatSpec multi-line comment in cluster 2.},
    label=lst:comment-cluster-2.1,
    language=Solidity]
/**
 * @dev See {IERC20-transfer}.
 *
 * Requirements:
 *
 * - `recipient` cannot be the zero address.
 * - the caller must have a balance of at least `amount`.
 * 
 * If recipient or sender is exchange, transaction will be taxed 1.7%
 * Tax is sent to our taxAddress
 */
\end{lstlisting}
\begin{lstlisting}[
    caption={NatSpec multi-line comment in cluster 2.},
    label=lst:comment-cluster-2.2,
    language=Solidity]
/**
 * @dev Creates a new token for `to`. Its token ID will be automatically
 * assigned (and available on the emitted {IERC721-Transfer} event), and the token
 * URI is specified as a parameter.
 *
 * See {ERC721-_mint}.
 *
 * Requirements:
 *
 * - the caller must have the `MINTER_ROLE`.
 */   
\end{lstlisting}

%  Categorical comments. Labels thte function withing a specific category. Eg. admin functions or 
\begin{lstlisting}[
    caption={Single-line comment in cluster 3.},
    label=lst:comment-cluster-3.1,
    language=Solidity]
// ----- admin functions -----
\end{lstlisting}
\begin{lstlisting}[
    caption={Multi-line comment in cluster 3.},
    label=lst:comment-cluster-3.2,
    language=Solidity]
/* ---------- Deposit Management ---------- */
\end{lstlisting}

\clearpage %\FloatBarrier but text is cut off at the end of the page

\subsection{Language Modeling}
\label{sec:language-modeling}
This chapter presents a detailed overview of the system architecture for generating secure Smart Contract code. The first section gives an overview of the GPT-J model architecture, followed by a section describing the pre-training of the model. The third section describes the fine-tuning process on the smart contract dataset presented in \cref{sec:verified-smart-contracts-inflated,sec:verified-smart-contracts-audit}. 

\todo{Add figure of training process}

\subsubsection{Pre-training}
\label{sec:pretraining}
In this project, pre-trained weights for GPT-J-6B from ElutherAI are used. The pre-training by ElutherAI is done on the dataset The Pile, described in \cref{sec:the-pile}. Of the roughly 825GiB, 95.16 GiB (7.59\%) of The Pile is code from GitHub. Compared to many other open-source models, GPT-J-6B is one of the most promising models for the task of code generation.

The specific GPT-J model configuration can be seen in \cref{tab:gpt-j-model-details}. In detail, GPT-J-6B consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. \acrfull{rope} is applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of \acrfullpl{bpe} as GPT-2 and GPT-3. The weights of GPT-J-6B are licensed under version 2.0 of the Apache License.

%Total params: 6,050,882,784
%multi-layer perceptron (MLP)

\begin{table}
    %\newcolumntype{Y}{>{\centering\arraybackslash}X}
    \def\arraystretch{1.5}
    \small
    \centering
    \caption{GPT-J-6B model details.}
    \label{tab:gpt-j-model-details}
    \begin{tabularx}{\textwidth}{XX}
        \toprule
        \textbf{Hyper parameter} & \textbf{Value}\\
        \midrule
        n\_parameters & 6,053,381,344\\
        n\_layers & 28*\\
        d\_model & 4,096\\
        d\_ff & 16,384\\
        n\_heads & 16\\
        d\_head & 256\\
        n\_ctx & 2,048\\
        n\_vocab & 50,257 (same tokenizer as GPT-2/3)\\
        position \& encoding & \acrfullpl{rope}\\
        RoPE dimensions & 64\\
        \bottomrule
    \end{tabularx}
\end{table}

\todo{add table notes}
* each layer consists of one feedforward block and one self attention block

\subsubsection{Fine-tuning}
\label{sec:fine-tuning}
To improve the pre-trained GPT-J-6B model's smart contract code generation performance, the model is fine-tuned on a dataset only containing real Ethereum Smart Contract code. Specifically, two models are created. The first model, named GPT-J-6B-Smart-Contract, is fine-tuned on the Verified Smart Contracts dataset \cref{sec:verified-smart-contracts}. The other model, named GPT-J-6B-Smart-Contract-Audit, is a secure version of the first model. It is fine-tuned on the Verified Smart Contracts Audit dataset \cref{sec:verified-smart-contracts-audit}, the same dataset as for the first model but with additional labeling from vulnerability analysis.

Elaborate on the different scripts and how HuggingFace was used for training. This  also includes the use of DeepSpeed...
Add link to github repos....

\todo{Add graphs of trining loss and eval accuraccy (results).}

\begin{table}
    %\newcolumntype{Y}{>{\centering\arraybackslash}X}
    \def\arraystretch{1.5}
    \small
    \centering
    \caption{Hyper parameters for GPT-J model}
    \label{tab:inclusion-exclusion-criteria}
    \begin{tabularx}{\textwidth}{XX}
        \toprule
        \textbf{Hyper parameter} & \\
        \midrule
        \_name\_or\_path & EleutherAI/gpt-j-6B\\
        activation\_function & gelu\_new\\
        architectures & GPTJForCausalLM\\
        attn\_pdrop & 0.0\\
        bos\_token\_id & 50256\\
        embd\_pdrop & 0.0\\
        eos\_token\_id & 50256\\
        gradient\_checkpointing & false\\
        initializer\_range & 0.02\\
        layer\_norm\_epsilon & 1e-05\\
        model\_type & gptj\\
        n\_embd & 4096\\
        n\_head & 16\\
        n\_inner & null\\
        n\_layer & 28\\
        n\_positions & 2048\\
        resid\_pdrop & 0.0\\
        rotary & true\\
        rotary\_dim & 64\\
        scale\_attn\_weights & true\\
        summary\_activation & null\\
        summary\_first\_dropout & 0.1\\
        summary\_proj\_to\_labels & true\\
        summary\_type & cls\_index\\
        summary\_use\_proj & true\\
        tie\_word\_embeddings & false\\
        tokenizer\_class & "GPT2Tokenizer"\\
        transformers\_version & "4.19.0.dev0"\\
        use\_cache & true\\
        vocab\_size & 50400\\
        \bottomrule
    \end{tabularx}
\end{table}

\begin{table}
    %\newcolumntype{Y}{>{\centering\arraybackslash}X}
    \def\arraystretch{1.5}
    \small
    \centering
    \caption{DeepSpeed Zero config.}
    \label{tab:inclusion-exclusion-criteria}
    \begin{tabularx}{\textwidth}{XX}
        \toprule
        \textbf{Hyper parameter} & \\
        \midrule
        stage & 2\\
        contiguous\_gradients & true\\
        reduce\_scatter & true\\
        reduce\_bucket\_size & 2.000000e+08\\
        allgather\_partitions & true\\
        allgather\_bucket\_size & 2.000000e+08\\
        overlap\_comm & true\\
        load\_from\_fp32\_weights & true\\
        elastic\_checkpoint & false\\
        offload\_param & null\\
        \midrule
        offload\_optimizer & device: null\\
        & nvme\_path: null\\
        & buffer\_count: 4\\
        & pin\_memory: false\\
        & pipeline\_read: false\\
        & pipeline\_write: false\\
        & fast\_init: false\\
        \midrule
        sub\_group\_size & 1.000000e+09\\
        prefetch\_bucket\_size & 5.000000e+07\\
        param\_persistence\_threshold & 1.000000e+05\\
        max\_live\_parameters & 1.000000e+09\\
        max\_reuse\_distance & 1.000000e+09\\
        gather\_16bit\_weights\_on\_model\_save & false\\
        ignore\_unused\_parameters & true\\
        round\_robin\_gradients & false\\
        legacy\_stage1 & false\\
        \bottomrule
    \end{tabularx}
\end{table}

%\subsection{Inference}
%
%TODO: What is supported by the model? How much memory to use?
%We perform beam search with width of 5 and optimize for accuracy@1

\FloatBarrier

\section{Implementation of RQ2}
This section presents the implementation of research question 2. The implementation is done with the following steps:
\begin{enumerate}
    \item Create an audited version of the smart contract dataset
    \begin{enumerate}
        \item Label the smart contracts with a vulnerability detection tool.
    \end{enumerate}
    \item Language modeling
    \begin{enumerate}
        \item Fine-tune a transformer model on the audited verified smart contract dataset, employing security conditioning.
    \end{enumerate}
\end{enumerate}

\subsection{Security Conditioning}
\label{sec:security-conditioning}
When training a large language model on several gigabytes of open-source code, it is safe to assume that large portions of this code are not safe and contains vulnerabilities. In the case of Smart Contracts, the vulnerability analysis presented in section \ref{sec:verified-smart-contracts} shows that almost 50\% of deployed Smart Contracts contain at least one high-severity vulnerability. This will result in a biased model that may produce a lot of vulnerable code. This section introduces a technique, named security conditioning, to reduce and mitigate this problem.

Vulnerability analysis is a difficult area. It is especially hard in the area of smart contracts, where the execution environment is not deterministic ????\todo{find correct wording.}... Previous works have tried to classify vulnerable code with large language models without much success \todo{Cite previous works}. In this project, instead of classifying vulnerable code, the goal is to make the model more secure by conditioning it on the presence of vulnerabilities.

The security conditioning is done by appending a special security label to each of the records in the training data. This way, the model can use this token(s) to condition whether to produce safe or vulnerable code. This requires the dataset to first be labeled as secure or vulnerable. For this project, SolidityDetector is used for labeling. Further details on the dataset construction can be found in \cref{sec:verified-smart-contracts-audit}.


\todo{Add example of security conditioning.}



\subsection{Data preparation}
\label{sec:data-preparation}

\subsubsection{Vulnerability labeling}
\label{sec:vulnerability-labeling}

\subsubsection{Verified Smart Contracts Audit}
\label{sec:verified-smart-contracts-audit}

\url{https://github.com/andstor/verified-smart-contracts-audit}
\url{https://huggingface.co/datasets/andstor/smart_contracts_audit}


Use  SoliDetector for labeling the dataset. The choice of using SoliDetector is due to its ability to any solidity version. As discussed in \cref{sec:vulnerability-detection}, several of the other vulnerability detection tools have many shortcomings. Both in terms of speed and accuracy.....More...

Describe figures and some simple analysis of the amount of vulnerable Ethereum code. Also, state the dataset used is balanced by 50/50.

\begin{figure}[htbp]
    \centering
    \input{figures/flattened_security_levels.pgf}
    \caption{Doughnut chart over the distribution of the vulnerability severities in the flattened dataset at different granularity levels, where each level occurs at least once in the \acrshort{sc}. The outer ring shows the additional security levels for each contract. For example, "HML" means that the contract has at least three vulnerabilities with the corresponding "High", "Medium", and "Low" security levels.}
\end{figure}


\begin{figure}[htbp]
    \centering
    \input{figures/flattened_vulnerabilities_bar.pgf}
    \caption{Distribution of vulnerabilities in the flattened dataset.}
\end{figure}



\begin{figure}[htbp]
    \centering
    \input{figures/inflated_security_levels.pgf}
    \caption{Doughnut chart over the distribution of the vulnerability severities in the inflated dataset at different granularity levels, where each level occurs at least once in the \acrshort{sc}. The inner ring shows the distribution of the occurrences of each level. The outer ring shows the additional security levels for each contract. For example, "HML" means that the contract has at least three vulnerabilities with the corresponding "High", "Medium", and "Low" security levels.}
\end{figure}


\begin{figure}[htbp]
    \centering
    \input{figures/inflated_vulnerabilities_bar.pgf}
    \caption{Distribution of vulnerabilities in the inflated dataset.}
\end{figure}

\subsection{Language Modeling}
\label{sec:language-modeling}

\subsubsection{Fine-tuning}
\label{sec:fine-tuning}

\todo{Write short description of the fine-tuning process. Same as in the previous project.}