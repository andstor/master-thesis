% Or call it Research results?

\chapter{Research Implementation and Results}
\label{chap:implementation-and-results}
This chapter presents the research implementation and results of the research questions. The chapter is divided into two parts. First, the implementation of research question 1 is described, concerning automatic smart contract code synthesis. The part of the chapter describes the implementation of research question 2, regarding generating secure smart contract code.

\section{Implementation of RQ1}
This section presents the implementation of research question 1. The implementation is done with the following steps:
\begin{enumerate}
    \item Create verified smart contract source code dataset.
    \begin{enumerate}
        \item Scrape verified smart contracts from the Ethereum blockchain.
        \item Normalize the smart contract files.
        \item Filter the smart contracts for uniqueness.
    \end{enumerate}
    \item Code comment analysis.
    \begin{enumerate}
        \item Create a parser that can parse all contract versions.
        \item Parse verified smart contract source code.
        \item Create a parsed dataset containing "comment, function" pairs.
        \item Cluster comments.
    \end{enumerate}
    \item Language modeling
    \begin{enumerate}
        \item Fine-tune a transformer model on the verified smart contracts dataset.
    \end{enumerate}
\end{enumerate}

\subsection{Data collection}
\label{sec:data-collection}

\subsubsection{Smart contract downloader}
\label{sec:smart-contract-downloader}

The largest provider of verified \acrshortpl{sc} is Etherscan. At \url{https://etherscan.io/} users can upload the source code for their deployed \acrshort{sc}. Etherscan will then compile this source code and verify that it matches the bytecode of the deployed \acrshort{sc} on the Ethereum blockchain. This way, other people can verify the functionality of a \acrshort{sc} before using it. Etherscan provides a simple \acrshort{api} for downloading verified Smart Contracts. The API is available at \url{https://api.etherscan.io/api}. From this endpoint, one can ask for the verified source code of a specific \acrshort{sc} address. However, it is not guaranteed that the contract has been verified.

The following code snippet is a Google BigQuery query. It selects all \acrshortpl{sc} addresses on the Ethereum blockchain that has at least one transaction. This query was run on the 1st of April 2022, and the result was downloaded as a CSV file, available on request at \url{https://huggingface.co/datasets/andstor/smart_contracts/blob/main/contract_addresses.csv}. The CSV file is then used to download the \acrshortpl{sc} from Etherscan.

\begin{lstlisting}[
    caption={Google BigQuery query for selecting all \acrlong{sc} addresses on Ethereum that has at least one transaction.},
    label=lst:reentrancy,
    language=SQL]
SELECT contracts.address, COUNT(1) AS tx_count
FROM `bigquery-public-data.crypto_ethereum.contracts` AS contracts
JOIN `bigquery-public-data.crypto_ethereum.transactions` AS transactions 
      ON (transactions.to_address = contracts.address)
GROUP BY contracts.address
ORDER BY tx_count DESC
\end{lstlisting}

%Saved to file for simple restarting, multiprocessing and parallelization.

The total number of files generated by the downloading program (\url{https://github.com/andstor/smart-contract-downloader}) was 5,810,042. In order to efficiently process these, all files were combined into a tarfile. A processing script was then created for filtering out all "empty" files. These correspond to a contract address on Ethereum that has not been verified on Etherscan.io. A total of 3,592,350 files were empty, making the source code of 38,17\% of the deployed contracts on Ethereum available. Each non-empty file is then parsed and the contract data is extracted. This extraction process is rather complicated, as smart contract sources come in a wide variety of flavors and formats. 

\paragraph{Normalization of smart contract files}
\label{par:normalization}
\acrshortpl{sc} come in multiple flavors. To avoid confusing the machine learning model, all training data should use the same format. Hence, all contract files are normalized. The most common format is a contract written the Solidity language with a single contract entry in the file. However, a single contract file can contain multiple contracts, making use of properties like inheritance etc. The source code contracts can also be split over multiple files, a format referred to as "Multi file". When compiling these, the source code files are "flattened" into a single contract file before compilation. Another flavor is the JSON format, which is a language that is used to describe the \acrshortpl{sc}. As can be seen in \cref{lst:standard-json-format}, here the source code is structured inside JSON code. Smart contracts can also be written in the Vyper language. Vyper is Pythonic programming language. Compared to Solidity, it has deliberately fewer features, making contracts more secure and easier to edit \cite{vyper}. However, it is much less popular than Solidity.

\begin{lstlisting}[
    caption={Solidity standard JSON Input format.},
    label=lst:standard-json-format,
    language=JSON]
{
    "sources": {/* ... */},
    "settings": {
        "optimizer": {/* ... */},
        "evmVersion": "<VERSION>"
    }
}
\end{lstlisting}

All of the above formats are processed by the processing script, normalizing the contract source code to a single "flattened" contract file. The source code, along with the contract metadata, is then saved across multiple Parquet files, each consisting of 30000 "flattened" contracts. A total of 2,217,692 smart contracts were successfully parsed and normalized.

\paragraph{Filter smart contracts for uniqueness}
\label{sec:duplication-filtering}
A large quantity of Smart Contracts contains duplicated code. Primarily, this is due to the frequent use of library code, such as SafeMath \cite{safemath} by OpenZeppelin \cite{openzeppelin}. Etherscan requires the library code used in a contract to be embedded in the source code. Filtering is applied to produce a dataset with a mostly unique contract source code to mitigate this. This is very important when used for model training. Failure to produce a sufficiently unique dataset would result in poor performance of a machine learning model, as it would overfit on similar data. The filtering is done by calculating the string distance between the source code. Due to the rather large amount of contracts (\~2 million), the comparison is only made within groups of contracts. These groups are defined by grouping on the "contract\_name" for the \textit{flattened} dataset, and by "file\_name" for the \textit{inflated} dataset. These datasets will be discussed in detail in the following sections.

The actual code filtering is done by applying a token-based similarity algorithm named Jacard Index, described in \cref{sec:jaccard-index}. The algorithm is computationally efficient and can be used to filter out \acrshortpl{sc} that are not similar to the query.

\subsubsection{Verified Smart Contracts}
\label{sec:verified-smart-contracts}
The Verified Smart Contracts dataset is a dataset consisting of verified Smart Contracts from Etherscan.io. These are real \acrshortpl{sc} that are deployed to the Ethereum blockchain, containing primarily Solidity and a very small fraction Vyper code. The dataset contains multiple subsets. In the following paragraphs, these subsets are described in detail. It consists of every deployed Ethereum Smart Contract as of 1st of April 2022, whose been verified on Etherescan.io and has a least one transaction. \cref{tab:verified-smart-contracts-metrics} shows the metrics of the various subsets. All processing scripts are available at \url{https://github.com/andstor/verified-smart-contracts}. The dataset is available on request at \url{https://huggingface.co/datasets/andstor/verified-smart-contracts}.

\begin{table}
    %\newcolumntype{Y}{>{\centering\arraybackslash}X}
    \def\arraystretch{1.5}
    \small
    \centering
    \caption{Verified Smart Contracts Metrics}
    \label{tab:verified-smart-contracts-metrics}
    \begin{tabularx}{\textwidth}{XXXX}
        \toprule
        \textbf{Component} & \textbf{Size} &  \textbf{Num rows} & \textbf{\acrshort{loc}}\\
        \midrule
        Raw & 0.80 GiB & 2,217,692 & 839,665,295\\
        Flattened & 1.16 GiB & 136,969 & 97,529,473\\
        Inflated & 0.76 GiB & 186,397 & 53,843,305\\
        \bottomrule
    \end{tabularx}
\end{table}

\paragraph{Raw}
\label{sec:verified-smart-contracts-raw}
The raw dataset contains mostly the raw data from Etherscan, downloaded with the smart-contract-downlader tool, as described in \cref{sec:smart-contract-downloader}. All different contract formats (JSON, multi-file, etc.) are normalized to a flattened source code structure, as described in \cref{par:normalization}.

\paragraph{Flattened}
\label{sec:verified-smart-contracts-flattened}

The flattened dataset is a filtered version of the Raw dataset. It contains smart contracts, where every contract contains all required library code. Each "file" is marked in the source code with a comment stating the original file path: //File: path/to/file.sol. These are then filtered for uniqueness with a similarity threshold of 0.9,  calculated using the Jacard index. This means that all contracts whose code shares more than 90\% of the tokens will be discarded. The low uniqueness requirement is due to the often large amount of embedded library code. If the requirement is set to high, the actual contract code will be negligible compared to the library code. Most contracts will be discarded, and the resulting dataset would contain mostly unique library code. However, the dataset as a whole will have a large amount of duplicated library code. From the 2,217,692 contracts, 2,080,723 duplications are found, giving a duplication percentage of 93.82\%. The resulting dataset consists of 136,969 contracts. \cref{lst:flattened-dataset-instance} shows an example data instance from the dataset. The dataset is then split 80\%, 10\%, 10\% into a training, validation and test set, respectively.

%Processing: 100%|██████| 74/74 [20:22<00:00, 16.51s/it, dupes=2081081/2217692 (93.84%)]
%
%The following command prooduces the flattened dataset:
%
%\lstinline[language=Python]!python script/filter_data.py -s parquet -o data/flattened --threshold 0.9!


\begin{lstlisting}[
    float,floatplacement=H!,
    caption={Example data instance from the flattened dataset.},
    label=lst:flattened-dataset-instance,
    language=JSON]
{
  'contract_name': 'MiaKhalifaDAO',
  'contract_address': '0xb3862ca215d5ed2de22734ed001d701adf0a30b4',
  'language': 'Solidity',
  'source_code': '// File: @openzeppelin/contracts/utils/Strings.sol\r\n\r\n\r\n// OpenZeppelin Contracts v4.4.1 (utils/Strings.sol)\r\n\r\npragma solidity ^0.8.0;\r\n\r\n/**\r\n * @dev String operations.\r\n */\r\nlibrary Strings {\r\n...',
  'abi': '[{"inputs":[{"internalType":"uint256","name":"maxBatchSize_","type":"uint256"}...]',
  'compiler_version': 'v0.8.7+commit.e28d00a7',
  'optimization_used': False,
  'runs': 200,
  'constructor_arguments': '000000000000000000000000000000000000000000000000000000000000000a000...',
  'evm_version': 'Default',
  'library': '',
  'license_type': 'MIT',
  'proxy': False,
  'implementation': '',
  'swarm_source': 'ipfs://e490df69bd9ca50e1831a1ac82177e826fee459b0b085a00bd7a727c80d74089'
}
\end{lstlisting}

\paragraph{Inflated}
\label{sec:verified-smart-contracts-inflated}
The inflated dataset is also based on the raw dataset. Each contract file in the dataset is split into its original representative files and hence  "inflated". This mitigates a lot of the problems of the flattened dataset in terms of duplicated library code. The library code is, along with other imported contract files, split (read inflated) into separate contract records. The 2,217,692 "raw" smart contracts are inflated to a total of 5,403,136 separate contract files. These are then grouped by "file\_name" and filtered for uniqueness with a similarity threshold of 0.9. This should produce a dataset with a large amount of unique source code, with low quantities of library code. A total of 5,216,739 duplications are found, giving a duplication percentage of 96.56\%. The resulting dataset consists of 186,397 contracts. \cref{lst:inflated-dataset-instance} shows an example data instance from the inflated dataset. The dataset is then split 80\%, 10\%, 10\% into a training, validation and test set, respectively.

%Processing: 100%|██████| 74/74 [22:50<00:00, 18.52s/it, dupes=5217191/5403136 (96.56%)]


%\lstinline[language=Python]!python script/filter_data.py -s parquet -o data/inflated --split-files --threshold 0.9!
%dupes=5217191/5403136 (96.56%)


\begin{lstlisting}[
    float,floatplacement=H!,
    caption={Example data instance from the inflated dataset.},
    label=lst:inflated-dataset-instance,
    language=JSON]
{
    'contract_name': 'PinkLemonade',
    'file_path': 'PinkLemonade.sol',
    'contract_address': '0x9a5be3cc368f01a0566a613aad7183783cff7eec',
    'language': 'Solidity',
    'source_code': '/**\r\n\r\nt.me/pinklemonadecoin\r\n*/\r\n\r\n// SPDX-License-Identifier: MIT\r\npragma solidity ^0.8.0;\r\n\r\n\r\n/*\r\n * @dev Provides information about the current execution context, including the\r\n * sender of the transaction and its data. While these are generally available...',
    'abi': '[{"inputs":[],"stateMutability":"nonpayable","type":"constructor"}...]',
    'compiler_version': 'v0.8.4+commit.c7e474f2',
    'optimization_used': False,
    'runs': 200,
    'constructor_arguments': '',
    'evm_version': 'Default',
    'library': '',
    'license_type': 'MIT',
    'proxy': False,
    'implementation': '',
    'swarm_source': 'ipfs://eb0ac9491a04e7a196280fd27ce355a85d79b34c7b0a83ab606d27972a06050c'
}
\end{lstlisting}

\paragraph{Plain text}
\label{sec:verified-smart-contracts-plain-text}
For easy use of the dataset for casual language modeling training with HuggingFace, a "plain\_text" version of both the raw, the flattened, and the inflated dataset is made available. This is done through a custom builder script for the dataset, a feature of the Dataset library by Hugging Face. \cref{lst:inflated-dataset-instance-plain-text} shows an example data instance of the "plain\_text" version.


\begin{lstlisting}[
    caption={Example data instance from the plain-text version of the inflated dataset.},
    label=lst:inflated-dataset-instance-plain-text,
    language=JSON]
{
    'language': 'Solidity',
    'text': 'pragma solidity =0.5.16;\r\n\r\n// a library for performing overflow-safe math...'
}
\end{lstlisting}

%\todo{fix tokens}
%smart\_contracts\_dataset :: inflated\_plain\_text tokens:
%	test: 74314082
%	validation: 67064318
%	train: 687186308
%TOTAL: 828.564.708

\FloatBarrier

\subsection{Code comment analysis}
\label{sec:comment-analysis}
To provide some insight into how a user can best formulate a comment for guiding the code synthesis, a cluster analysis of the comments in the smart contract dataset is conducted. First, a universal Solidity parser is constructed for parsing the Solidity code and extracting "code, comment" pairs. These results are then packaged into a dataset, and a clustering analysis is conducted. The results from this analysis are then later used in the evaluation of the code synthesis in \cref{chap:evaluation} to shedd some light on which commenting style is the best to use.

\subsubsection{Universal Solidity parser}
\label{sec:universal-solidity-parser}
To parse the Solidity \acrshort{sc}, a Solidity parser is constructed. This parser has to be universally compatible with all Solidity versions, hence the grammar used needs to be a lot less restrictive than the current official Solidity grammar available from Ethereum \cite{soliditygrammar}. ANTLR4 \cite{antlr4} is used for constructing the parser. ANTLR is a parser generator. By providing ANTLR with a formal language description called grammar, it can generate a complete parser that can automatically build parse trees. Parse trees are data structures representing how the grammar matches the input. Specifically, ANTLR4 generates a LL(*) (Left-to-right, leftmost derivation) parser \cite{parr2011llstar}. ANTLR is primarily a Java application. However, several code generation targets are available, including Java, C\#, Python, JavaScript, Go, C++, Swift, PHP and Dart \cite{antlr-targets}. In this project, the Python target is used.

Most programming language grammars available do not devote much effort to the handling of code comments. Comments are seen as unnecessary clutter and are normally discarded during lexing. For extracting the comments from the Solidity \acrshort{sc} code, the original source \cite{solidity-antlr4} for the official Solidity grammar \cite{soliditygrammar} is used. This old version is less restrictive and serves as a better starting point for ensuring support for all Solidity versions. This grammar is then simplified and made less restrictive, as well as adapted to support comments. \cref{fig:solidity-railroad-diagram} shows a railroad diagram of a subset of the main grammar rules altered for supporting comments. The complete universal Solidity parser is made available at https://github.com/andstor/solidity-universal-parser.

\begin{figure}[htbp]
    \centering
    %\makebox[\textwidth][c]{\includegraphics{figures/solidity-comments-railroad-diagram.pdf}}%
    \includegraphics[width=\textwidth]{figures/solidity-comments-railroad-diagram.pdf}
    \caption{Railroad diagrams of main code comment alteration to Solidity grammar.}
    \label{fig:solidity-railroad-diagram}
\end{figure}


\subsubsection{Verified Smart Contract Code Comments}
\label{sec:verified-smart-contract-code-comments}
For doing the actual extraction of the "code, comment" pairs from the inflated version of the Verified Smart Contracts dataset (see \cref{sec:verified-smart-contracts-inflated}), the well-known visitor pattern \cite{visitor-pattern} is used for visiting the parse tree generated by the universal Solidity parser. ANTLER provides basic infrastructure for implementing such a visitor. The full implementation of the visitor is available at \url{https://github.com/andstor/verified-smart-contracts/blob/main/script/comment_visitor.py}. A script leveraging multiprocessing is used to parallelize the parsing of the dataset. See \url{https://github.com/andstor/verified-smart-contracts} for instructions on how to use this script. The resulting data is then filtered for functions that do not have code comments. These are simply removed and the result is then packaged as a new dataset named Verified Smart Contract Code Comments. A total of 1.541.370 functions are extracted. \cref{lst:comments-dataset-instance} shows an example data instance from the dataset. The dataset is available on request at \url{https://huggingface.co/datasets/andstor/smart_contract_comments}.

%The results are then packaged and added to the Verified Smart Contracts dataset collection (\cref{sec:verified-smart-contracts}) under the name "parsed".Verified Smart Contracts dataset collection (\cref{sec:verified-smart-contracts}).

%Parsed: Total 4434014 functions, 4.44 GiB and 29965185 lines of code
%\lstinline[language=Python]!python script/parse_data.py -s data/inflated -o data/parsed!

\begin{lstlisting}[
    float,floatplacement=H!,
    caption={Example data instance from the inflated dataset.},
    label=lst:comments-dataset-instance,
    language=JSON]
{
    'contract_name': 'BondedECDSAKeep',
    'file_path': '@keep-network/keep-core/contracts/StakeDelegatable.sol',
    'contract_address': '0x61935dc4ffc5c5f1d141ac060c0eef04a792d8ee',
    'language': 'Solidity',
    'class_name': 'StakeDelegatable',
    'class_code': 'contract StakeDelegatable {\n    using OperatorParams for uint256;\n\n    mapping(address => Operator) internal operators;\n\n    struct Operator {\n        uint256 packedParams;\n        address owner;\n        address payable beneficiary;\n        address authorizer;\n    }\n\n...',
    'class_documentation': '/// @title Stake Delegatable\n/// @notice A base contract to allow stake delegation for staking contracts.',
    'class_documentation_type': 'NatSpecSingleLine',
    'func_name': 'balanceOf',
    'func_code': 'function balanceOf(address _address) public view returns (uint256 balance) {\n        return operators[_address].packedParams.getAmount();\n    }',
    'func_documentation': '/// @notice Gets the stake balance of the specified address.\n/// @param _address The address to query the balance of.\n/// @return An uint256 representing the amount staked by the passed address.',
    'func_documentation_type': 'NatSpecSingleLine',
    'compiler_version': 'v0.5.17+commit.d19bba13',
    'license_type': 'MIT',
    'swarm_source': 'bzzr://63a152bdeccda501f3e5b77f97918c5500bb7ae07637beba7fae76dbe818bda4'
}  
\end{lstlisting}

\subsubsection{Comment clustering}
\label{sec:comment-clustering}
This section is devoted to the clustering of the comments in the parsed dataset. The comments in the dataset are first preprocessed. In contrast to normal code, code comments are of a more natural language style. Normal natural language text preprocessing is therefore employed. First, the comments are lowercased and tokenized. The default English configuration of the \lstinline[language=Python]!word_tokenize! function from the popular \acrfull{nltk} \cite{nltk} python library is used for tokenization. Stemming is applied to the tokenized words, using the Porter Stemmer algorithm.

For converting the tokenized comments into word embeddings, both the word2vec algorithm \cite{word2vec} and \acrfull{tfidf} is used. The word2vec is able to capture some semantic similarities between the words. In particular, the implementation provided by the gensim library \cite{gensim} is used. The algorithm is configured to produce 100-dimensional vectors, using a window size of 5, and a minimum count of 5. 
%Word2Vec min_count=5,
%                     window=5,
%                     vector_size=100,
%                     sample=0.001,
%                     seed=1,
%                     alpha=0.025,
%                     min_alpha=0.0001, 
%                     negative=5,

To weed out the most frequent words \acrshort{tfidf} is also applied. For example, the different commenting types all start each line with a special word, such as "//", "///" or "*". By using \acrshort{tfidf}, it is possible to get more insights into the different ways of writing comments, beyond just the formatting style of the comments. The resulting word embeddings from the word2vec and \acrshort{tfidf} are multiplied. For each comment, the resulting word embeddings are averaged to form a final comment (or document) embedding.

The comment embeddings are clustered using the K-means algorithm. The number of clusters \(k\) is determined by using the Elbow method for deciding the optimal number of clusters. The results from the Elbow method are presented in \cref{fig:elbow}. From the curve, it is not entirely obvious where the "elbow" is. However, a \(k\) of 4 is selected. For visually inspecting the clustered comments result, the 100-dimensional vectors are reduced to 2D using \acrfull{pca}. The explained variance captured in the 2D plot is approximately 0.64, as shown in the Scree Plot in \cref{fig:scree-plot}. The clustering result is shown in \cref{fig:comment-clusters}.

%Word2Vec lifecycle event {'msg': 'training on 1431483720 raw words (762458270 effective words) took 782.9s, 973910 effective words/s', 'datetime': '2022-06-07T00:40:05.999678', 'gensim': '4.2.0', 'python': '3.7.13 (default, Apr 24 2022, 01:04:09) \n[GCC 7.5.0]', 'platform': 'Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}
%Time to train the model: 13.05 mins

\begin{figure}[htbp]
    \centering
    \input{figures/elbow.pgf}
    \caption{Elbow method for determining the optimal number of clusters.}
    \label{fig:elbow}
    \vspace*{\floatsep}% https://tex.stackexchange.com/q/26521/5764
    \input{figures/pca_explained_variance.pgf}
    \caption{Scree Plot for the PCA dimensionality reduction}
    \label{fig:scree-plot}
\end{figure}


\begin{figure}[htbp]
    \centering
    \input{figures/2d_cluster_marginals.pgf}
    \caption{2D plot of the comment clusters.}
    \label{fig:comment-clusters}
\end{figure}

\cref{lst:comment-cluster-0,lst:comment-cluster-1,lst:comment-cluster-2,lst:comment-cluster-3} shows an example from each of the four clusters. Upon manual inspection of the different clusters, several patterns emerge. Cluster 0 is mainly composed of comments that is almost exclusively made up of NatSpec comments with only NatSpec fields, for example the "@parameter" and "@return" fields. Most comments also start with a brief description of the function, as for example line 1 in \cref{lst:comment-cluster-0}. Next, cluster 1 consists of one-liners, briefly describing what the function. Cluster 2 contains more lengthy comments that describe the function in detail. It is similar to cluster 1 but does not make significant use of the NatSpec fields. Several of these comments are from some implementation of common libraries. For example, \cref{lst:comment-cluster-2} shows a comment for the implementation of a transfer function in a contract implementation of a ERC20 token. Compared to the base implementation by the OpenZeppelin library \cite{openzeppelin}, this version adds 1.7\% tax if the sender or recipient is an exchange (lines 8-10). Finally, cluster 3 contains that presents a more "artistic" nature. For example, \cref{lst:comment-cluster-3} marks the start and end of the comments with many dashes.


% Natspec with almost exclusive use of parameter and return keywords.
\begin{lstlisting}[
    caption={NatSpec single-line comment in cluster 0.},
    label=lst:comment-cluster-0,
    language=Solidity]
/// @dev Executes the next transaction only if the cooldown has passed and the transaction has not expired
/// @param to Destination address of module transaction
/// @param value Ether value of module transaction
/// @param data Data payload of module transaction
/// @param operation Operation type of module transaction
/// @notice The txIndex used by this function is always 0
\end{lstlisting}

% One-liners describing briefly the function.
\begin{lstlisting}[
    caption={Single-line comment in cluster 1.},
    label=lst:comment-cluster-1,
    language=Solidity]
// Allow the owner to cash out the holdings of this contract.
\end{lstlisting}

% Library function implementations. The functions are clearly written, often stating  the requirements of the function. Written out in natural  language.
\begin{lstlisting}[
    caption={NatSpec multi-line comment in cluster 2.},
    label=lst:comment-cluster-2,
    language=Solidity]
/**
 * @dev See {IERC20-transfer}.
 *
 * Requirements:
 *
 * - `recipient` cannot be the zero address.
 * - the caller must have a balance of at least `amount`.
 * 
 * If recipient or sender is exchange, transaction will be taxed 1.7%
 * Tax is sent to our taxAddress
 */
\end{lstlisting}

%  Categorical comments. Labels thte function withing a specific category. Eg. admin functions or 
\begin{lstlisting}[
    caption={Custom comment style from cluster 3},
    label=lst:comment-cluster-3,
    language=Solidity]
// ------------------------------------------------------------------------
// Returns the amount of tokens approved by the owner that can be
// transferred to the spender's account
//
// THIS TOKENS ARE NOT TRANSFERRABLE.
//
// ------------------------------------------------------------------------
\end{lstlisting}

\clearpage %\FloatBarrier but text is cut off at the end of the page

\subsection{Language Modeling}
\label{sec:rq1-language-modeling}
This section presents a detailed overview of the system architecture for generating \acrlong{sc} code. The first section describes the specific configuration of the pre-trained model used. Following is a section that describes the fine-tuning process on the inflated Verified Smart Contract dataset presented in \cref{sec:verified-smart-contracts-inflated}.

\subsubsection{Pre-training}
\label{sec:pretraining}
In this project, pre-trained weights for GPT-J-6B from EleutherAI are used. See \cref{sec:architecture} for a description of the model  architecture. The pre-training by EleutherAI is done on the dataset The Pile, described in \cref{sec:the-pile}. Of the roughly 825GiB, 95.16 GiB (7.59\%) of The Pile is code from GitHub. Compared to many other open-source models, GPT-J-6B is one of the most promising models for the task of code generation.

The specific GPT-J model configuration can be seen in \cref{tab:gpt-j-model-details}. In detail, GPT-J-6B consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. \acrfull{rope} is applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of \acrfullpl{bpe} as GPT-2 and GPT-3. The weights of GPT-J-6B are licensed under version 2.0 of the Apache License. When assessed on the validation split of the inflated Verified Smart Contract dataset \cref{sec:verified-smart-contracts-inflated}, the model achieves an accuracy of 0.800 and a perplexity of 2.600.

%Total params: 6,050,882,784
%multi-layer perceptron (MLP)

\begin{table}
    %\newcolumntype{Y}{>{\centering\arraybackslash}X}
    \def\arraystretch{1.5}
    \small
    \centering
    \caption{GPT-J-6B model details.}
    \label{tab:gpt-j-model-details}
    \begin{tabularx}{\textwidth}{XX}
        \toprule
        \textbf{Hyperparameter} & \textbf{Value}\\
        \midrule
        n\_parameters & 6,053,381,344\\
        n\_layers & 28\\
        d\_model & 4,096\\
        d\_ff & 16,384\\
        n\_heads & 16\\
        d\_head & 256\\
        n\_ctx & 2,048\\
        n\_vocab & 50,257 (same tokenizer as GPT-2/3)\\
        position \& encoding & \acrfullpl{rope}\\
        RoPE dimensions & 64\\
        \bottomrule
    \end{tabularx}
\end{table}

\subsubsection{Fine-tuning}
\label{sec:rq1-fine-tuning}
To improve the pre-trained GPT-J-6B model's smart contract code generation performance, the model is fine-tuned on a dataset only containing real Ethereum Smart Contract code. Specifically, the model is fine-tuned on the training split of the plain-text \cref{sec:verified-smart-contracts-plain-text} version of the inflated Verified Smart Contracts dataset \cref{sec:verified-smart-contracts-inflated}. The fine-tuning task used is the same as for the pre-training task, namely \acrfull{clm}. The model is fed a complete \acrshort{sc} all at once, and then internal masking is applied to prevent the model from cheating by looking at future tokens. For more details on the inner workings of the training procedure, see \cref{sec:transformer-training}. Before training, the dataset is randomly shuffled. For running the training process, the \acrshort{clm} script \footnote{\url{https://github.com/huggingface/transformers/blob/v4.19.0/examples/pytorch/language-modeling/run_clm.py}} provided by HuggingFace is used.

Due to the huge size of the GPT-J-6B model, the deep learning optimization library DeepSpeed \cite{deepspeed} is used as a wrapper around the HuggingFace library. See \cref{par:deepspeed} for more details of the DeepSpeed library. While DeepSpeed enables the training of virtually arbitrary-sized models, there is a tradeoff between model size and training speed. In this project, several DeepSpeed configurations were tried out to successfully load and train the model without encountering an \acrfull{oom} error, while still maintaining adequate training speed. Using \acrshort{zero}-2 with CPU offloading (\acrshort{zero}-Offload), mixed-precision (\acrshort{bf16}), a batch size of 1, and 16 gradient accumulation steps, it is possible to load and efficiently train the model using 10 x NVIDIA A100 GPUs with 40GB memory \footnote{\url{https://www.nvidia.com/en-us/data-center/a100/}}. The computing node used for training has 48 CPUs available, along with 1.47 terabytes of RAM. \cref{fig:nvidia-smi} shows a screenshot of the nvidia-smi program during the training of the model. As can be seen from the figure, all GPUs are at 100\% utilization. \cref{fig:htop-optimizer-computation} presents a screenshot of the htop program showing host CPU and memory activity during optimizer computation. The command for running the HuggingFace training script while using DeepSpeed is shown in \cref{lst:training-command}. A complete list of the hyperparameters used for training the model is available in \cref{tab:model-hyperparameters}, along with the DeepSpeed configuration in \cref{tab:deepspeed-config}. All training scripts and configurations used are available at \url{https://github.com/andstor/smart-contract-code-generation}.

The training process is run for two epochs. At every 5 steps, the model is evaluated on 256 samples from the validation split of the Verified Smart Contracts dataset. \cref{fig:wandb-train-loss-gpt-j-smart-contract} shows a graph over the training and evaluation loss during training. \cref{fig:wandb-train-eval-gpt-j-smart-contract} shows a graph over the evaluation accuracy during training. The training is completed after 7 days and 4s hours. After completion of the training, the model is evaluated on the entire validation split, achieving a total accuracy of 0.917 and perplexity of 1.510. The fine-tuned model is available on request at \url{https://huggingface.co/andstor/gpt-j-6B-smart-contract}.

\begin{lstlisting}[
    caption={Command for running the HuggingFace \acrshort{clm} training script with DeepSpeed.},
    label=lst:training-command,
    language=Bash]
deepspeed --hostfile=hostfile run_clm.py \
    --deepspeed ds_zero2_bf16.json \
    --model_name_or_path EleutherAI/gpt-j-6B \
    --dataset_name andstor/smart_contracts \
    --dataset_config_name plain_text \
    --output_dir ./out \
    --report_to wandb \
    --validation_split_percentage 20 \
    --save_steps 250 \
    --do_train --do_eval \
    --logging_first_step --logging_steps 1 \
    --num_train_epochs 2 \
    --evaluation_strategy steps --eval_steps 5 \
    --max_eval_samples 256 \
    --block_size 1024 \
    --bf16 \
    --gradient_accumulation_steps 16 --eval_accumulation_steps 16 \
    --per_device_train_batch_size 1 --per_device_eval_batch_size 1
\end{lstlisting}

\begin{table}
    %\newcolumntype{Y}{>{\centering\arraybackslash}X}
    \def\arraystretch{1.5}
    \small
    \centering
    \caption{Hyperparameters for GPT-J model}
    \label{tab:model-hyperparameters}
    \begin{tabularx}{\textwidth}{XX}
        \toprule
        \textbf{Hyperparameter} & \\
        \midrule
        learning\_rate & 5e-05\\
        train\_batch\_size & 1\\
        eval\_batch\_size & 1\\
        seed & 42\\
        distributed\_type & multi-GPU\\
        num\_devices & 10\\
        gradient\_accumulation\_steps & 16\\
        total\_train\_batch\_size & 160\\
        total\_eval\_batch\_size & 10\\
        optimizer & Adam with betas=(0.9,0.999) and epsilon=1e-08\\
        lr\_scheduler\_type & linear\\
        num\_epochs & 2.0\\
        \bottomrule
    \end{tabularx}
\end{table}
%\begin{table}
%    %\newcolumntype{Y}{>{\centering\arraybackslash}X}
%    \def\arraystretch{1.5}
%    \small
%    \centering
%    \caption{Hyperparameters for GPT-J model}
%    \label{tab:model-hyperparameters}
%    \begin{tabularx}{\textwidth}{XX}
%        \toprule
%        \textbf{Hyperparameter} & \\
%        \midrule
%        \_name\_or\_path & EleutherAI/gpt-j-6B\\
%        activation\_function & gelu\_new\\
%        architectures & GPTJForCausalLM\\
%        attn\_pdrop & 0.0\\
%        bos\_token\_id & 50256\\
%        embd\_pdrop & 0.0\\
%        eos\_token\_id & 50256\\
%        gradient\_checkpointing & false\\
%        initializer\_range & 0.02\\
%        layer\_norm\_epsilon & 1e-05\\
%        model\_type & gptj\\
%        n\_embd & 4096\\
%        n\_head & 16\\
%        n\_inner & null\\
%        n\_layer & 28\\
%        n\_positions & 2048\\
%        resid\_pdrop & 0.0\\
%        rotary & true\\
%        rotary\_dim & 64\\
%        scale\_attn\_weights & true\\
%        summary\_activation & null\\
%        summary\_first\_dropout & 0.1\\
%        summary\_proj\_to\_labels & true\\
%        summary\_type & cls\_index\\
%        summary\_use\_proj & true\\
%        tie\_word\_embeddings & false\\
%        tokenizer\_class & "GPT2Tokenizer"\\
%        transformers\_version & "4.19.0.dev0"\\
%        use\_cache & true\\
%        vocab\_size & 50400\\
%        \bottomrule
%    \end{tabularx}
%\end{table}

\begin{table}
    %\newcolumntype{Y}{>{\centering\arraybackslash}X}
    \def\arraystretch{1.5}
    \small
    \centering
    \caption{DeepSpeed Zero configuration.}
    \label{tab:deepspeed-config}
    \begin{tabularx}{\textwidth}{XX}
        \toprule
        \textbf{Hyperparameter} & \\
        \midrule
        stage & 2\\
        contiguous\_gradients & true\\
        reduce\_scatter & true\\
        reduce\_bucket\_size & 2.000000e+08\\
        allgather\_partitions & true\\
        allgather\_bucket\_size & 2.000000e+08\\
        overlap\_comm & true\\
        load\_from\_fp32\_weights & true\\
        elastic\_checkpoint & false\\
        cpu\_offload\ & true\\
        sub\_group\_size & 1.000000e+09\\
        prefetch\_bucket\_size & 5.000000e+07\\
        param\_persistence\_threshold & 1.000000e+05\\
        max\_live\_parameters & 1.000000e+09\\
        max\_reuse\_distance & 1.000000e+09\\
        gather\_16bit\_weights\_on\_model\_save & false\\
        ignore\_unused\_parameters & true\\
        round\_robin\_gradients & false\\
        legacy\_stage1 & false\\
        \bottomrule
    \end{tabularx}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/nvidia-smi.png}
    \caption{Screenshot of nvidia-smi program showing 100\% GPU utilization.}
    \label{fig:nvidia-smi}
\end{figure}
%37565+36133+34881+36101+35659+35591+35571+35579+35595+36555  = 359230MB ~ 360GiB

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/htop-optimizer-computation.png}
    \caption{Screenshot of htop program showing host CPU and memory activity during optimizer computation.}
    \label{fig:htop-optimizer-computation}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figures/wandb-train-loss-gpt-j-smart-contract.png}
    \caption{Training and evaluation loss during model training.}
    \label{fig:wandb-train-loss-gpt-j-smart-contract}

    \vspace*{\floatsep}% https://tex.stackexchange.com/q/26521/5764

    \includegraphics[width=\textwidth]{figures/wandb-train-eval-gpt-j-smart-contract.png}
    \caption{Evaluation accuracy during model training.}
    \label{fig:wandb-train-eval-gpt-j-smart-contract}
\end{figure}
%\subsection{Inference}
%
%TODO: What is supported by the model? How much memory to use?
%We perform beam search with width of 5 and optimize for accuracy@1

\FloatBarrier

\section{Implementation of RQ2}
This section presents the implementation of research question 2. Primarily, this section describes the implementation of the new technique named security conditioning, described in \cref{sec:security-conditioning}. In security conditioning, the training data is augmented with security labels, stating either secure or vulnerable. The main alteration needed is in the training data. The implementation is done with the following steps:
\begin{enumerate}
    \item Create an audited version of the smart contract dataset
    \begin{enumerate}
        \item Label the smart contracts with a vulnerability detection tool.
    \end{enumerate}
    \item Language modeling
    \begin{enumerate}
        \item Fine-tune a transformer model on the audited verified smart contract dataset, employing security conditioning.
    \end{enumerate}
\end{enumerate}

\subsection{Data preparation}
\label{sec:data-preparation}

\subsubsection{Vulnerability labeling}
\label{sec:vulnerability-labeling}
For labeling the \acrshortpl{sc} as vulnerable or secure, the Java program SolDetector by \textcite{soldetector} is used. The choice of using SolDetector is due for two reasons. Firstly, as SolDetector is ontology-based, it does not need a complete contract file with all code dependencies. This makes it possible to use the inflated dataset version (see \cref{sec:verified-smart-contracts-inflated}). Other vulnerability detection tools such as \textsc(Oyente) \cite{oyente2016making} that for example use symbolic analysis, would only work on the flattened dataset version (see \cref{sec:verified-smart-contracts-flattened}). SolDetector works on both. Secondly, SolDetector works with any Solidity version.

SolDetector takes in a \acrshort{sc} file and outputs the vulnerability analysis results as a text file. In this text file, the detected vulnerability type(s) and the offending line(s) are reported. The paper \cite{soldetector} of SolDetector also classifies the different vulnerabilities according to risk level. Due to the large number of contracts needed to be labeled in this project, SolDetector is run in parallel. A python script is created that leverages multiprocessing to run SolDetector in parallel. Since SolDetector is a Java program, it is run as a child process and controlled with the help of the python module Pexpect \cite{pexpect}. Since starting and stopping Java applications are time-consuming, extra care is taken to ensure that each instantiated Solidity process is kept alive for as long as possible and only restarted when necessary. \cref{fig:solidetector-vulnerability-labeling} shows a screenshot from running the processing script using 40 processes. The vulnerability processing scripts are available at \url{https://github.com/andstor/verified-smart-contracts-audit}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/solidetector-run.png}
    \caption{Screenshot from the vulnerability labeling process with SolDetector.}
    \label{fig:solidetector-vulnerability-labeling}
\end{figure}

\subsubsection{Verified Smart Contracts Audit dataset}
\label{sec:verified-smart-contracts-audit}
The results of the vulnerability labeling are packed as subsets into a dataset named Verified Smart Contracts Audit. This is done for both the flattened and inflated dataset versions. The finished dataset is available on request at \url{https://huggingface.co/datasets/andstor/smart_contracts_audit}. Both dataset versions keep the original split into a training, validation and test set (80\%, 10\%, 10\%). \cref{lst:audit-inflated-dataset-instance} shows an example data instance from the audited inflated dataset.

%\todo{Add total token count}
%smart\_contracts\_dataset-audit :: inflated\_solidetector\_embedded tokens:
%	test: 74332721
%	validation: 67082958
%	train: 687335426
%TOTAL: 828.751.105


\begin{lstlisting}[
    caption={Example data instance from the audited inflated dataset.},
    label=lst:audit-inflated-dataset-instance,
    language=JSON]
{
    'contract_name': 'OceanWorld',
    'file_path': 'OceanWorld.sol',
    'contract_address': '0xe19c5ea08f26af53bf7da7da5e727bb2c5c69f95',
    'language': 'Solidity',
    'source_code': 'pragma solidity ^0.8.0; contract OceanWorld is ERC721Enumerable...',
    'defects': '[{"defect": "Nest_Call", "type": "Business_logic", "severity": "High", "lines": ["193", "125", "165"]}, {"defect": "Frozen_Ether", "type": "Code_specification", "severity": "Medium", "lines": ["3"]}, {"defect": "Exceed_authority_access", "type": "Access_control", "severity": "Medium", "lines": ["31"]}]',
    'compiler_version': 'v0.8.7+commit.e28d00a7',
    'license_type': 'MIT',
    'swarm_source': 'ipfs://36f4cbcbeca01a804a52ae73931c970301e46d79022cdf26e6e6158d9105fe83'
}
\end{lstlisting}

\cref{fig:flattened_security_levels} shows a doughnut chart over the distribution of the vulnerability severities in the flattened dataset at different granularity levels, where each level occurs at least once in the \acrshort{sc}. The outer ring shows the additional security levels for each contract. For example, "HML" means that the contract has at least three vulnerabilities with the corresponding "High", "Medium", and "Low" security levels. As can be seen in the figure, almost three-quarters of the contracts contain at least one high-risk vulnerability. \cref{fig:flattened_vulnerabilities_bar} shows the distribution of the different types of vulnerabilities in the flattened dataset, categorized by severity level. Notably, a significant portion of the high-severity vulnerabilities is integer overflow and underflow vulnerabilities. \cref{fig:inflated_security_levels,fig:inflated_vulnerabilities_bar} presents the same vulnerability distribution chart for the audited contracts in the inflated dataset. The distribution of vulnerability types follows the same characteristics as for the flattened dataset. However, only around half of the contracts contain at least one high-risk vulnerability. As described in \cref{sec:verified-smart-contracts-inflated}, the main intention behind the inflated dataset is to reduce the amount of library. Hence, one can deduce that a significant portion of the vulnerabilities come from various \acrshort{sc} libraries.

\paragraph{Embedded}
\label{sec:verified-smart-contracts-audit-embedded}
For easy use of the labeled dataset for \acrfull{clm} training, an
"embedded" version of both the flattened and the inflated dataset is made
available. This is done through a custom builder script for the dataset, a feature
of the Dataset library by Hugging Face. The builder script parses the contract audit and determines whether the contract is secure or vulnerable. Based on this analysis, it then prepends "<|secure|>" or "<|vulnerable|>" to the top of the contract source code. In this project, a contract is considered secure if it does not contain any high-risk vulnerabilities. Otherwise, the contract is considered vulnerable. This also makes the inflated dataset balanced, as about 50\% of the contracts are secure and 50\% are vulnerable (see \cref{fig:inflated_security_levels}).

\begin{figure}[htbp]
    \centering
    \input{figures/flattened_security_levels.pgf}
    \caption{Doughnut chart over the distribution of the vulnerability severities in the flattened dataset at different granularity levels, where each level occurs at least once in the \acrshort{sc}.}
    \label{fig:flattened_security_levels}
    \vspace*{\floatsep}% https://tex.stackexchange.com/q/26521/5764
    \input{figures/flattened_vulnerabilities_bar.pgf}
    \caption{Distribution of vulnerabilities in the flattened dataset.}
    \label{fig:flattened_vulnerabilities_bar}
\end{figure}

\begin{figure}[htbp]
    \centering
    \input{figures/inflated_security_levels.pgf}
    \caption{Doughnut chart over the distribution of the vulnerability severities in the inflated dataset at different granularity levels, where each level occurs at least once in the \acrshort{sc}.}
    \label{fig:inflated_security_levels}
    
    \vspace*{\floatsep}% https://tex.stackexchange.com/q/26521/5764

    \input{figures/inflated_vulnerabilities_bar.pgf}
    \caption{Distribution of vulnerabilities in the inflated dataset.}
    \label{fig:inflated_vulnerabilities_bar}
\end{figure}

\subsection{Language Modeling}
\label{sec:rq2-language-modeling}
This section presents the language modeling procedure using the security conditioning technique proposed in \cref{sec:security-conditioning} for generating secure \acrlong{sc} code. In security conditioning, the training data is augmented with security labels, stating either secure or vulnerable. This data augmentation is implemented by the embedded version of the Verified Smart Contracts Audit dataset \cref{sec:verified-smart-contracts-audit-embedded}, by adding "<|secure|>" or "<|vulnerable|>" to secure or vulnerable contracts. To make the most use of the security labels, a small alteration to the tokenizer is made, as described in the following section. Otherwise, the language modeling procedure is more or less identical to the one used for RQ1 \cref{sec:rq1-language-modeling}.

\subsubsection{Tokenizer}
Depending on the security labels and the type of tokenizer used, the tokenizer might decide to split the security label into multiple, already pre-trained, tokens. For example, the "<|secure|>" label is tokenized into five different tokens: '<', '|', 'secure', '|', '>' with corresponding ids: 27, 91, 22390, 91, 29. These tokens might also be part of making up other words. This might confuse the model during training, making it harder for it to successfully condition on the labels. To mitigate this, the security labels "<|secure|>" and "<|vulnerable|>" are added as special tokens to the tokenizer, effectively expanding the vocabulary. The "<|secure|>" label is now instead tokenized as "<|secure|>" with id 50400. This change also requires resizing the model's embedding matrix. The two added embeddings are randomly initialized.

\subsubsection{Fine-tuning}
For fine-tuning the model on the embedded version of the Verified Smart Contracts Audit dataset \cref{sec:verified-smart-contracts-audit-embedded}, the same procedure and hyperparameters as in RQ1 \label{sec:rq1-fine-tuning} are used. The training process is run for two epochs. At every 5 steps, the model is evaluated on 256 samples from the validation split of the Verified Smart Contracts Audit dataset. \cref{fig:wandb-train-loss-gpt-j-smart-contract-audit} shows a graph of the training and evaluation loss during training. \cref{fig:wandb-train-eval-gpt-j-smart-contract-audit} shows a graph over the evaluation accuracy during training. The training is completed after 7 days and 4 hours. After completion of the training, the model is evaluated on the entire validation split, achieving a total accuracy of 0.917 and perplexity of 1.510. Compared to the fine-tuned model without security conditioning (see \cref{sec:rq1-fine-tuning}), the technique does not introduce any significant performance decrease in terms of neither accuracy nor perplexity. The fine-tuned model is available on request at \url{https://huggingface.co/andstor/gpt-j-6B-smart-contract-audit}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figures/wandb-train-loss-gpt-j-smart-contract-audit.png}
    \caption{Training and evaluation loss during training of model with security conditioning.}
    \label{fig:wandb-train-loss-gpt-j-smart-contract-audit}

    \vspace*{\floatsep}% https://tex.stackexchange.com/q/26521/5764

    \includegraphics[width=\textwidth]{figures/wandb-train-eval-gpt-j-smart-contract-audit.png}
    \caption{Evaluation plot of accuracy during training of model with security conditioning.}
    \label{fig:wandb-train-eval-gpt-j-smart-contract-audit}
\end{figure}
